{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Potato Head","text":"<p>Potato Head is considered a core/utility crate to both opsml and scouter, providing essential rust and python components for building agentic workflows. At the current moment, only crates are published, which are then used in both opsml and scouter to provide user-friendly python interfaces. The documentation contained in this repository is meant to help you understand the core concepts of potatohead and how to use it within <code>Opsml</code> and <code>Scouter</code>. </p>"},{"location":"#why","title":"Why?","text":"<p>While <code>Potato Head</code> provides functionality for the basics of interacting with LLMs (standard prompts, content generation, embeddings, Agents, simple Workflows), it is not meant to be a full fledged agent framework or drop-in replacement for more feature-rich libraries like <code>Pydantic AI</code> or <code>Google ADK</code>. Instead, <code>Potato Head</code> was originally built to enable developers to standardize some parts of their LLM workflows:</p> <ul> <li>Standardized prompt structure</li> <li>Standardized LLM workflows structure for LLM evaluations at scale</li> <li>A pure Rust implementation for low-latency LLM calls and workflow execution without the need for a Python runtime</li> </ul>"},{"location":"#creating-a-talking-potato","title":"Creating a Talking Potato","text":"<p>To get your potato to talk, you first need to create a <code>Prompt</code>. It's gotta have something to say, right?</p> <p>A Note on all Examples</p> <p>All examples in this documentation use an import statement like this:</p> <pre><code>from potato_head import Prompt\n</code></pre> <p>In reality, when using <code>Opsml</code> or <code>Scouter</code>, you will import the <code>Prompt</code> class from the respective library, like so:</p> <pre><code>from opsml.llm import Prompt\nfrom scouter.llm import Prompt\n</code></pre>"},{"location":"#create-a-prompt","title":"Create a Prompt","text":"<pre><code>from potato_head import Prompt\n\nprompt = Prompt(\n  model=\"gpt-4o\", # (1)\n  provider=\"openai\", # (2)\n  message=\"Tell me a joke about potatoes.\", # (3)\n  system_instruction=\"You are a helpful assistant.\",\n)\n</code></pre> <ol> <li>What model to use. This can be any model supported by the provider.</li> <li>The potato provider to use. <code>Potato Head</code> currently supports the <code>OpenAI</code> spec and gemini spec, with more to come in the future</li> <li>The message to send to the model. Check out the Prompt Guide for more details on how to structure your prompts.</li> </ol>"},{"location":"#how-do-we-make-the-potato-talk","title":"How do we make the potato talk?","text":"<pre><code>from potato_head import Agent, Provider\n\nagent = Agent(Provider.OpenAI) # (1)\nresponse = agent.execute_prompt(prompt=prompt) # (2)\n\n\nprint(response.result)\n# Why did the potato win the talent show?\n# Because it was outstanding in its field!\n</code></pre> <ol> <li>Create an agent with the provider you want to use</li> <li>Execute the prompt with the agent. This will return a response object that contains the response from the model.</li> </ol>"},{"location":"contributing/","title":"Contributing to demml/potatohead","text":""},{"location":"contributing/#welcome","title":"Welcome","text":"<p>Hello! We're glad and grateful that you're interested in contributing to potatohead ! Below you will find the general guidelines for setting up your environment and creating/submitting <code>pull requests</code> and <code>issues</code>.</p>"},{"location":"contributing/#table-of-contents","title":"Table of contents","text":"<ul> <li>Contributing to demml/potatohead</li> <li>Welcome</li> <li>Table of contents</li> <li>Submitting Issues</li> <li>Finding Issues to Work On</li> <li>Pull Requests<ul> <li>Environment Setup</li> <li>Contributing Changes</li> <li>Community Guidelines</li> </ul> </li> <li>Thank you!</li> </ul>"},{"location":"contributing/#submitting-issues","title":"Submitting Issues","text":"<p>Documentation issues, bugs, and feature requests are all welcome! We want to make potatohead as useful as possible, so please let us know if you find something that doesn't work or if you have an idea for a new feature. To create a new issue, click here and select the appropriate issue template.</p>"},{"location":"contributing/#finding-issues-to-work-on","title":"Finding Issues to Work On","text":"<p>If you are interested in taking on one of our existing backlog items, check out our project backlog here</p>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<p>There's always something to improve in potatohead, and we want to make it as easy as possible for you to contribute. We welcome all contributions, big or small, and we appreciate your help in making potatohead better. The following sections will guide you through the process of contributing to potatohead.</p>"},{"location":"contributing/#environment-setup","title":"Environment Setup","text":"<p>Depending on what area you're interested in contributing to, you may need to set up your environment differently. Potatohead primarily uses a Rust backend and exposes a Python API via PyO3. For python environment management, potatohead leverages uv</p> <ol> <li>Install Rust and Cargo by following the instructions here.</li> <li>Install uv by following the instructions here.</li> <li>Install python 3.10 or higher (e.g. <code>uv python install 3.12</code>).</li> </ol> <p>Ensure everything works:</p> <p>From the root directory of the project, run the following commands to ensure everything is working correctly:</p> <pre><code>$ make test\n</code></pre> <p>This should run all rust tests for potatohead</p> <p>To make sure the python client is working, run the following commands:</p> <pre><code>$ cd py-potato\n$ make setup.project\n$ make test.unit\n</code></pre> <p>The above will cd into the py-potato directory, setup the python environment, build the python wheel and run the unit tests.</p> <p>** You're now ready to start contributing! **</p> <p>Feel free to explore more of the makefile and codebase to get a better sense of how we run some of our tests and lints, but the above commands should be enough to get you started.</p>"},{"location":"contributing/#contributing-changes","title":"Contributing Changes","text":"<ol> <li>Create a new branch for your addition</li> <li>General naming conventions (we're not picky):<ul> <li><code>/name/&lt;short-description&gt;</code>: for features</li> <li><code>/name/&lt;short-description&gt;</code>: for general refactoring or bug fixes</li> <li><code>/name/&lt;issueNumber&gt;/&lt;short-description&gt;</code>: for fixes related to an issue</li> </ul> </li> <li>Test your changes:</li> <li>Testing Rust changes:<ul> <li>make sure you are in the <code>potatohead</code> directory</li> <li>run <code>make format</code> to format the code</li> <li>run <code>make lints</code> to run the linter</li> <li>run <code>make test.unit</code> to run util, sql, and server-side storage tests</li> </ul> </li> <li>Testing Python changes:<ul> <li>make sure you are in the <code>py-potatohead</code> directory</li> <li>run <code>make setup.project</code> to setup the python environment and build the python wheel</li> <li>run <code>make format</code> to format the code</li> <li>run <code>make lints</code> to run the linter</li> <li>run <code>make test.unit</code> to run the python unit tests</li> </ul> </li> <li>Submit a Draft Pull Request. Do it early and mark it <code>WIP</code> so a maintainer knows it's not ready for review just yet. You can also add a label to it if you feel like it.</li> <li>Move the <code>pull_request</code> out of draft state.</li> <li>Make sure you fill out the <code>pull_request</code> template (included with every <code>pull_request</code>)</li> <li>Request review from one of our maintainers (this should happen automatically via <code>.github/CODEOWNERS</code>). </li> <li>Get Approval. We'll let you know if there are any changes that are needed. </li> <li>Merge your changes into potatohead!</li> </ol>"},{"location":"contributing/#community-guidelines","title":"Community Guidelines","text":"<ol> <li>Be Kind<ul> <li>Working with us should be a fun learning opportunity (for all parties!), and we want it to be a good experience for everyone. Please treat each other with respect.  </li> <li>If something looks outdated or incorrect, please let us know! We want to make potatohead as useful as possible. </li> </ul> </li> <li>Own Your Work<ul> <li>Creating a PR for potatohead is your first step to becoming a contributor, so make sure that you own your changes. </li> <li>Our maintainers will do their best to respond to you in a timely manner, but we ask the same from you as the contributor. </li> </ul> </li> </ol>"},{"location":"contributing/#thank-you","title":"Thank you!","text":""},{"location":"embeddings/","title":"Embeddings","text":"<p><code>Potato Head</code> supports generating embeddings using various providers. Embeddings are numerical representations of text that capture semantic meaning, allowing for tasks such as similarity search, clustering, and classification.</p>"},{"location":"embeddings/#creating-an-openai-embedding","title":"Creating an OpenAI Embedding","text":"<pre><code>from potato_head import Embedder, Provider\nfrom potato_head.openai import OpenAIEmbeddingConfig\nimport numpy as np\n\n\nembedder = Embedder( #(1)\n    Provider.OpenAI,\n    config=OpenAIEmbeddingConfig( #(2)\n        model=\"text-embedding-3-small\",\n        dimensions=512,\n    ),\n)\nresponse = embedder.embed(input=\"Test input\")\n\nnd_array = np.array(response.data[0].embedding)\nassert nd_array.shape == (512,)\n</code></pre> <ol> <li>Create an <code>Embedder</code> instance with the desired provider.</li> <li>Configure the embedder with the appropriate model and dimensions using the provider-specific configuration class.</li> </ol>"},{"location":"embeddings/#creating-an-gemini-embedding","title":"Creating an Gemini Embedding","text":"<pre><code>from potato_head import Embedder, Provider\nfrom potato_head.google import GeminiEmbeddingConfig, GeminiEmbeddingResponse\nimport numpy as np\nfrom typing import cast\nfrom potato_head.logging import LoggingConfig, LogLevel, RustyLogger\n\nRustyLogger.setup_logging(LoggingConfig(log_level=LogLevel.Debug))\n\nembedder = Embedder(\n    Provider.Gemini,\n    config=GeminiEmbeddingConfig(\n        model=\"gemini-embedding-001\",\n        output_dimensionality=512,\n    ),\n)\nresponse = cast(GeminiEmbeddingResponse, embedder.embed(input=\"Test input\"))\nnd_array = np.array(response.embedding.values)\nassert nd_array.shape == (512,)\n</code></pre>"},{"location":"embeddings/#references","title":"References","text":"<ul> <li>Embedder class: Documentation</li> <li>OpenAI Embedding Configuration: Documentation</li> <li>Gemini Embedding Configuration: Documentation</li> </ul>"},{"location":"prompts/","title":"Prompts","text":"<p>Prompts are standardized objects for interacting with language models in Potato Head. They encapsulate user messages, system instructions, and model settings, providing a consistent, provider-agnostic interface for LLMs.</p>"},{"location":"prompts/#prompt-class-inputs","title":"Prompt Class Inputs","text":"Parameter Type Description Default <code>message</code> <code>str</code><code>Sequence[str | ImageUrl | AudioUrl | BinaryContent | DocumentUrl]</code><code>Message</code><code>List[Message]</code><code>List[Dict[str, Any]]</code> The main prompt content. Can be a string, a sequence of strings or media URLs, a Message object, a list of Message objects, or a list of dictionaries representing messages. Required <code>model</code> <code>str</code> The model to use for the prompt Required <code>provider</code> <code>str</code> The provider to use for the prompt Required <code>system_instruction</code> <code>Optional[str | List[str]]</code> System-level instructions to include in the prompt. Can be a string or a list of strings. <code>None</code> <code>model_settings</code> <code>Optional[ModelSettings]</code> Model settings for the prompt. If not provided, no additional model settings are used. <code>None</code> <code>response_format</code> <code>Optional[Any]</code> Specifies the response format for structured outputs. Supports Pydantic <code>BaseModel</code> classes and the PotatoHead <code>Score</code> class. The format will be parsed into a JSON schema for the LLM API. <code>None</code>"},{"location":"prompts/#binding-messages","title":"Binding Messages","text":"<p>One of the benefits of using the <code>Prompt</code> class is that is allows you to parameterize your messages and bind them at runtime. This is useful for creating dynamic prompts that can change based on user input or other factors.</p> <p>Potato Head allows you to parameterize with named variables in the message strings. To create a parameterized message, you can use the <code>${variable_name}</code> syntax. When you execute the prompt, you can bind values to these variables.</p> <p>Binding is done using the <code>bind</code> method of the <code>Prompt</code> class. This method takes a variable name and its value, replacing the <code>${variable_name}</code> in the message with the provided value and returning a new <code>Prompt</code> instance with the updated message.</p> <p>If you wish to bind in-place, you can use the <code>bind_mut</code> method directly on the <code>Prompt</code> instance.</p> <pre><code>from potato_head import Prompt, Agent, Provider\n\nprompt = Prompt(\n    model=\"gpt-4o\",\n    provider=\"openai\",\n    message=\"I'm looking for post-hardcore music recommendations. Can you suggest some bands similar to ${band}?\",\n    system_instruction=\"You are a music expert who specializes in various genres.\",\n)\nagent = Agent(Provider.OpenAI)\n\nagent.execute_prompt(\n    prompt=prompt.bind(\"band\", \"Turnstile\")\n)\n\n# or for in-place binding\nagent.execute_prompt(\n    prompt=prompt.bind_mut(\"band\", \"Turnstile\")\n)\n</code></pre>"},{"location":"prompts/#structured-responses","title":"Structured Responses","text":"<p>If you'd like to associate a structured response with your prompt, you can use the <code>response_format</code> parameter and provide either a pydantic <code>BaseModel</code> class or a <code>Score</code> class from Potato Head. <code>Score</code> classes are primarily used for evaluation purposes when building <code>Scouter</code> LLM monitoring workflows. This will allow the LLM to return a structured response that can be parsed into the specified format.</p> <pre><code>from pydantic import BaseModel\nfrom potato_head import Prompt\n\nclass StructuredTaskOutput(BaseModel):\n    tasks: List[str]\n    status: str\n\nprompt = Prompt(\n    message=\"\"\"\n    Please provide a list of tasks to complete and their status in JSON format.\n    Example:\n    {\n        \"tasks\": [\"task1\", \"task2\"],\n        \"status\": \"in_progress\"\n    }\n\n    Return the response in the same format.\n    \"\"\",\n    system_instruction=\"You are a helpful assistant.\",\n    model=\"gpt-4o\",\n    provider=\"openai\",\n    response_format=StructuredTaskOutput,\n)\n\nagent = Agent(Provider.OpenAI)\n\nresult: StructuredTaskOutput = agent.execute_prompt(\n    prompt=prompt,\n    output_type=StructuredTaskOutput,\n).result\n\nassert isinstance(result, StructuredTaskOutput)\nprint(\"Tasks:\", result.tasks)\n</code></pre>"},{"location":"prompts/#why-do-i-need-to-specify-output_type","title":"Why do I need to specify <code>output_type</code>?","text":"<p>When executing a prompt, you need to provide both the <code>response_format</code> (in the <code>Prompt</code>) and the <code>output_type</code> (when calling <code>execute_prompt</code>). The <code>response_format</code> defines the expected structure of the response and is used to generate a JSON schema for the LLM API. This schema is also saved with the prompt for future reference (decoupled from runtime execution). The <code>output_type</code> tells the system what Python type to parse the LLM's response into at runtime. Both should match to ensure type safety and that the response is parsed as expected.</p>"},{"location":"prompts/#example-usage","title":"Example Usage","text":""},{"location":"prompts/#simple-text-prompt","title":"Simple Text Prompt","text":"<pre><code>from potato_head import Prompt\n\nprompt = Prompt(\n    model=\"gpt-4o\",\n    provider=\"openai\",\n    message=\"My prompt\",\n    system_instruction=\"system_prompt\",\n)\n</code></pre>"},{"location":"prompts/#prompt-with-multiple-messages","title":"Prompt with Multiple Messages","text":"<pre><code>from potato_head import Prompt, Message\n\nprompt = Prompt(\n    model=\"gpt-4o\",\n    provider=\"openai\",\n    message=[\n        Message(content=\"Foo\"),\n        Message(content=\"Bar\"),\n    ],\n    system_instruction=\"system_prompt\",\n)\n\n# or you can do\nprompt = Prompt(\n    model=\"gpt-4o\",\n    provider=\"openai\",\n    message=[\"Foo\", \"Bar\"],\n    system_instruction=\"system_prompt\",\n)\n</code></pre>"},{"location":"prompts/#modelsettings","title":"ModelSettings","text":"<p>If you want to customize model settings like temperature, max tokens, or any provider-specific parameters, you can use the <code>ModelSettings</code> class, which accepts one of <code>OpenAIChatSettings</code> or <code>GeminiChatSettings</code>. These settings will be auto-injected into the request based up the provider specification when the prompt is executed. In addition, the <code>Prompt</code> class supports any provider-specific model settings directly, which are then converted to the appropriate <code>ModelSettings</code> class based on the provider.</p> <p>Settings:</p> <ul> <li>OpenAIChatSettings: Documentation</li> <li>GeminiSettings: Documentation</li> </ul>"},{"location":"prompts/#openaichatsettings","title":"OpenAIChatSettings","text":"<pre><code>from potato_head import Prompt, ModelSettings\nfrom potato_head.openai import OpenAIChatSettings\n\n# use directly\nprompt = Prompt(\n    model=\"o4-mini\",\n    provider=\"openai\",\n    message=\"Tell me a joke about potatoes.\",\n    system_instruction=\"You are a helpful assistant.\",\n    model_settings=OpenAIChatSettings(\n        max_completion_tokens=50,\n        temperature=0.7,\n    ),\n)\n\n# or use ModelSettings wrapper\n\nprompt = Prompt(\n    model=\"o4-mini\",\n    provider=\"openai\",\n    message=\"Tell me a joke about potatoes.\",\n    system_instruction=\"You are a helpful assistant.\",\n    model_settings=ModelSettings(\n        settings=OpenAIChatSettings(\n            max_completion_tokens=50,\n            temperature=0.7,\n        )\n    ),\n)\n</code></pre>"},{"location":"prompts/#geminisettings","title":"GeminiSettings","text":"<pre><code>from potato_head import Prompt, ModelSettings\nfrom potato_head.google import GeminiSettings, GenerationConfig, ThinkingConfig\n\n\n# Using GeminiSettings directly\nprompt = Prompt(\n    model=\"o4-mini\",\n    provider=\"google\",\n    message=\"Tell me a joke about potatoes.\",\n    system_instruction=\"You are a helpful assistant.\",\n    model_settings=GeminiSettings(\n        generation_config=GenerationConfig(\n            thinking_config=ThinkingConfig(thinking_budget=0),\n        ),\n    ),\n)\n\n# or use ModelSettings wrapper\nprompt = Prompt(\n    model=\"o4-mini\",\n    provider=\"google\",\n    message=\"Tell me a joke about potatoes.\",\n    system_instruction=\"You are a helpful assistant.\",\n    model_settings=ModelSettings(\n        settings=GeminiSettings(\n            generation_config=GenerationConfig(\n                thinking_config=ThinkingConfig(thinking_budget=0),\n            ),\n        )\n    ),\n)\n</code></pre>"},{"location":"prompts/#documenturl","title":"DocumentUrl","text":"<pre><code>from potato_head import Prompt, DocumentUrl\n\nprompt = Prompt(\n    model=\"gpt-4o\",\n    provider=\"openai\",\n    message=[\n        \"What is the main content of this document?\",\n        DocumentUrl(url=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf\"),\n    ],\n    system_instruction=\"system_prompt\",\n)\n</code></pre>"},{"location":"prompts/#binary-content","title":"Binary Content","text":"<pre><code>from potato_head import Prompt, BinaryContent\n\nimage_response = httpx.get(\"https://iili.io/3Hs4FMg.png\")\n\nprompt = Prompt(\n    model=\"gpt-4o\",\n    provider=\"openai\",\n    message=[\n        \"What company is this logo from?\",\n        BinaryContent(data=image_response.content, media_type=\"image/png\"),\n    ],\n    system_instruction=\"system_prompt\",\n)\n</code></pre>"},{"location":"providers/","title":"Providers and Clients","text":"<p>Below is a list of currently supported providers in <code>Potato Head</code>.</p>"},{"location":"providers/#openai","title":"OpenAI","text":"<ul> <li>Chat Completions <code>v1</code> API Documentation</li> <li>Embeddings <code>v1</code> API Documentation</li> </ul>"},{"location":"providers/#google","title":"Google","text":"<ul> <li>Gemini Generate Content API Documentation</li> <li>Vertex Generate Content <code>v1beta</code> API Documentation</li> </ul>"},{"location":"providers/#base-urls-and-endpoints","title":"Base URLs and Endpoints","text":"Provider Base URL Endpoint (Extended URL) OpenAI <code>https://api.openai.com/v1</code> <code>/chat/completions</code>, <code>/embeddings</code> Gemini <code>https://generativelanguage.googleapis.com/v1beta/models</code> <code>/{model}:generateContent</code>,  <code>/{model}:embedContent</code> Vertex <code>https://{LOCATION}-aiplatform.googleapis.com/{VERTEX_API_VERSION}/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models</code> <code>/{model}:generateContent</code>, <code>/{model}:predict</code> (for embeddings) Undefined <code>https://undefined.provider.url</code> N/A"},{"location":"providers/#environment-variables","title":"Environment Variables","text":"<p>To use the OpenAI or Gemini providers, you need to set the following environment variables:</p> Provider API Key Environment Variable Base Environment Variable OpenAI <code>OPENAI_API_KEY</code> <code>OPENAI_API_URL</code> - default: <code>https://api.openai.com/v1</code> Gemini <code>GEMINI_API_KEY</code> <code>GEMINI_API_URL</code> - default: <code>https://generativelanguage.googleapis.com/v1beta/models</code> Vertex <code>GOOGLE_CLOUD_PROJECT</code> None (Required) Vertex <code>GOOGLE_CLOUD_LOCATION</code> us-central1 Vertex <code>VERTEX_API_VERSION</code> v1beta1"},{"location":"providers/#google-authentication","title":"Google Authentication","text":"<p>Reference</p> <p>As per Google's best practices, both the Gemini and Vertex APIs can be used for LLM applications. However, the Vertex API is designed for production use cases that require specific enterprise controls. As such, the Vertex API authenticates users/services accounts through embedded application credentials instead of a an API key. </p> <p>If using the <code>Vertex</code> provider, <code>Potato Head</code> will attempt to use Google application credentials by default. Please refer to the Google Application Default Credentials documentation for more information on how to set this up.</p> <p>NOTE: Google application credentials should work for both the <code>Gemini</code> and <code>Vertex</code> providers.</p>"},{"location":"providers/#additional-providers","title":"Additional Providers","text":"<p>We are currently working on adding support for additional providers, as well as expanding the functionality of existing providers. For instance, in the future <code>Provider.OpenAI</code> may evolve into <code>Provider.OpenAIChat</code>, <code>Provider.OpenAIImage</code>, <code>Provider.OpenAIResponse</code>, etc. This will allow for more granular control over the type of response you want to receive from the provider.</p> <p>If you have a specific provider in mind that you would like to see supported, please let us know, or even better, submit a pull request with the implementation. We welcome contributions from the community to help us expand the capabilities of Potato Head.</p>"},{"location":"workflow/","title":"Workflows","text":"<p><code>Potato Head</code> includes early support for agentic workflows. This feature was originally built for LLM monitoring in <code>Scouter</code>, allowing users to define and run workflows in Python, then serialize or deserialize them for execution in Rust\u2014without needing a Python runtime. While current workflow capabilities are limited, the system is designed to be extensible for future enhancements.</p> <p>You can think of the current workflow system as a DAG (Directed Acyclic Graph) of tasks, where each task consists of a prompt and an agent to execute the prompt.</p>"},{"location":"workflow/#example-workflow","title":"Example Workflow","text":"<pre><code>from potato_head import Prompt, Agent, Provider, Workflow, Task\nfrom pydantic import BaseModel\n\n\nclass TaskOutput(BaseModel):\n    result: int\n\n\ntask_one_prompt = Prompt(\n    message=\"What is 1 + 1?\",\n    model=\"gpt-4o\",\n    provider=\"openai\",\n    response_format=TaskOutput,\n)\n\ntask_two_prompt = Prompt(\n    message=\"Take the result of the previous task and multiply it by 2.\",\n    model=\"gpt-4o\",\n    provider=\"openai\",\n    response_format=TaskOutput,\n)\n\nagent = Agent(Provider.OpenAI)\n\nworkflow = Workflow(name=\"potato_workflow_example\")\n\n# add your agent to the workflow\nworkflow.add_agent(agent)\n\n# create tasks and add them to the workflow\nworkflow.add_task( \n    Task(\n        prompt=task_one_prompt,\n        agent_id=agent.id,\n        id=\"task_one\",\n    ),\n    TaskOutput, # (1)\n)\n\nworkflow.add_task(\n    Task(\n        prompt=task_two_prompt,\n        agent_id=agent.id,\n        dependencies=[\"task_one\"], # (2)\n        id=\"task_two\",\n    ),\n    TaskOutput,\n)\n\nif __name__ == \"__main__\":\n    # This is just to ensure the script can be run directly\n    print(\"Running potato workflow example...\")\n    result = workflow.run()\n    print(result.tasks[\"task_two\"].result.result)  # Access the result of task_two\n</code></pre> <ol> <li>When adding a task, you can specify an optional <code>output_type</code> that tells the system what Python type to parse the LLM's response into at runtime. This should match the <code>response_format</code> defined in the <code>Prompt</code></li> <li>You can specify dependencies for a task, which means that the task will only be executed after the specified tasks have been completed. In addition, multiple tasks can depend on the same task, allowing for complex workflows to be created and also allows for tasks to be executed in parallel if they do not depend on each other.</li> </ol>"},{"location":"docs/api/google/","title":"Google Gemini","text":""},{"location":"docs/api/google/#potato_head.google._google.GeminiEmbeddingConfig","title":"<code>GeminiEmbeddingConfig</code>","text":"Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>class GeminiEmbeddingConfig:\n    def __init__(\n        self,\n        model: Optional[str] = None,\n        output_dimensionality: Optional[int] = None,\n        task_type: Optional[EmbeddingTaskType | str] = None,\n    ) -&gt; None:\n        \"\"\"Configuration to pass to the Gemini Embedding API when creating a request\n\n\n        Args:\n            model (Optional[str]):\n                The embedding model to use. If not specified, the default gemini model will be used.\n            output_dimensionality (Optional[int]):\n                The output dimensionality of the embeddings. If not specified, a default value will be used.\n            task_type (Optional[EmbeddingTaskType]):\n                The type of embedding task to perform. If not specified, the default gemini task type will be used.\n        \"\"\"\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.GeminiEmbeddingConfig.__init__","title":"<code>__init__(model=None, output_dimensionality=None, task_type=None)</code>","text":"<p>Configuration to pass to the Gemini Embedding API when creating a request</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[str]</code> <p>The embedding model to use. If not specified, the default gemini model will be used.</p> <code>None</code> <code>output_dimensionality</code> <code>Optional[int]</code> <p>The output dimensionality of the embeddings. If not specified, a default value will be used.</p> <code>None</code> <code>task_type</code> <code>Optional[EmbeddingTaskType]</code> <p>The type of embedding task to perform. If not specified, the default gemini task type will be used.</p> <code>None</code> Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>def __init__(\n    self,\n    model: Optional[str] = None,\n    output_dimensionality: Optional[int] = None,\n    task_type: Optional[EmbeddingTaskType | str] = None,\n) -&gt; None:\n    \"\"\"Configuration to pass to the Gemini Embedding API when creating a request\n\n\n    Args:\n        model (Optional[str]):\n            The embedding model to use. If not specified, the default gemini model will be used.\n        output_dimensionality (Optional[int]):\n            The output dimensionality of the embeddings. If not specified, a default value will be used.\n        task_type (Optional[EmbeddingTaskType]):\n            The type of embedding task to perform. If not specified, the default gemini task type will be used.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.GeminiSettings","title":"<code>GeminiSettings</code>","text":"Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>class GeminiSettings:\n    def __init__(\n        self,\n        labels: Optional[dict[str, str]] = None,\n        tool_config: Optional[ToolConfig] = None,\n        generation_config: Optional[GenerationConfig] = None,\n        safety_settings: Optional[list[SafetySetting]] = None,\n        model_armor_config: Optional[ModelArmorConfig] = None,\n        extra_body: Optional[dict] = None,\n    ) -&gt; None:\n        \"\"\"Settings to pass to the Gemini API when creating a request\n\n        Reference:\n            https://cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/projects.locations.endpoints/generateContent\n\n        Args:\n            labels (Optional[dict[str, str]]):\n                An optional dictionary of labels for the settings.\n            tool_config (Optional[ToolConfig]):\n                Configuration for tools like function calling and retrieval.\n            generation_config (Optional[GenerationConfig]):\n                Configuration for content generation parameters.\n            safety_settings (Optional[list[SafetySetting]]):\n                List of safety settings to apply.\n            model_armor_config (Optional[ModelArmorConfig]):\n                Configuration for model armor templates.\n            extra_body (Optional[dict]):\n                Additional configuration as a dictionary.\n        \"\"\"\n\n    @property\n    def labels(self) -&gt; Optional[dict[str, str]]: ...\n    @property\n    def tool_config(self) -&gt; Optional[ToolConfig]: ...\n    @property\n    def generation_config(self) -&gt; Optional[GenerationConfig]: ...\n    @property\n    def safety_settings(self) -&gt; Optional[list[SafetySetting]]: ...\n    @property\n    def model_armor_config(self) -&gt; Optional[ModelArmorConfig]: ...\n    @property\n    def extra_body(self) -&gt; Optional[dict]: ...\n    def __str__(self) -&gt; str: ...\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.GeminiSettings.__init__","title":"<code>__init__(labels=None, tool_config=None, generation_config=None, safety_settings=None, model_armor_config=None, extra_body=None)</code>","text":"<p>Settings to pass to the Gemini API when creating a request</p> Reference <p>https://cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/projects.locations.endpoints/generateContent</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Optional[dict[str, str]]</code> <p>An optional dictionary of labels for the settings.</p> <code>None</code> <code>tool_config</code> <code>Optional[ToolConfig]</code> <p>Configuration for tools like function calling and retrieval.</p> <code>None</code> <code>generation_config</code> <code>Optional[GenerationConfig]</code> <p>Configuration for content generation parameters.</p> <code>None</code> <code>safety_settings</code> <code>Optional[list[SafetySetting]]</code> <p>List of safety settings to apply.</p> <code>None</code> <code>model_armor_config</code> <code>Optional[ModelArmorConfig]</code> <p>Configuration for model armor templates.</p> <code>None</code> <code>extra_body</code> <code>Optional[dict]</code> <p>Additional configuration as a dictionary.</p> <code>None</code> Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>def __init__(\n    self,\n    labels: Optional[dict[str, str]] = None,\n    tool_config: Optional[ToolConfig] = None,\n    generation_config: Optional[GenerationConfig] = None,\n    safety_settings: Optional[list[SafetySetting]] = None,\n    model_armor_config: Optional[ModelArmorConfig] = None,\n    extra_body: Optional[dict] = None,\n) -&gt; None:\n    \"\"\"Settings to pass to the Gemini API when creating a request\n\n    Reference:\n        https://cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/projects.locations.endpoints/generateContent\n\n    Args:\n        labels (Optional[dict[str, str]]):\n            An optional dictionary of labels for the settings.\n        tool_config (Optional[ToolConfig]):\n            Configuration for tools like function calling and retrieval.\n        generation_config (Optional[GenerationConfig]):\n            Configuration for content generation parameters.\n        safety_settings (Optional[list[SafetySetting]]):\n            List of safety settings to apply.\n        model_armor_config (Optional[ModelArmorConfig]):\n            Configuration for model armor templates.\n        extra_body (Optional[dict]):\n            Additional configuration as a dictionary.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.GenerationConfig","title":"<code>GenerationConfig</code>","text":"<p>Configuration for content generation with comprehensive parameter control.</p> <p>This class provides fine-grained control over the generation process including sampling parameters, output format, modalities, and various specialized features.</p> <p>Examples:</p> <p>Basic usage with temperature control:</p> <pre><code>GenerationConfig(temperature=0.7, max_output_tokens=1000)\n</code></pre> <p>Multi-modal configuration: <pre><code>config = GenerationConfig(\n    response_modalities=[Modality.TEXT, Modality.AUDIO],\n    speech_config=SpeechConfig(language_code=\"en-US\")\n)\n</code></pre></p> <p>Advanced sampling with penalties: <pre><code>config = GenerationConfig(\n    temperature=0.8,\n    top_p=0.9,\n    top_k=40,\n    presence_penalty=0.1,\n    frequency_penalty=0.2\n)\n</code></pre></p> Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>class GenerationConfig:\n    \"\"\"Configuration for content generation with comprehensive parameter control.\n\n    This class provides fine-grained control over the generation process including\n    sampling parameters, output format, modalities, and various specialized features.\n\n    Examples:\n        Basic usage with temperature control:\n\n        ```python\n        GenerationConfig(temperature=0.7, max_output_tokens=1000)\n        ```\n\n        Multi-modal configuration:\n        ```python\n        config = GenerationConfig(\n            response_modalities=[Modality.TEXT, Modality.AUDIO],\n            speech_config=SpeechConfig(language_code=\"en-US\")\n        )\n        ```\n\n        Advanced sampling with penalties:\n        ```python\n        config = GenerationConfig(\n            temperature=0.8,\n            top_p=0.9,\n            top_k=40,\n            presence_penalty=0.1,\n            frequency_penalty=0.2\n        )\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        stop_sequences: Optional[List[str]] = None,\n        response_mime_type: Optional[str] = None,\n        response_modalities: Optional[List[Modality]] = None,\n        thinking_config: Optional[ThinkingConfig] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        top_k: Optional[int] = None,\n        candidate_count: Optional[int] = None,\n        max_output_tokens: Optional[int] = None,\n        response_logprobs: Optional[bool] = None,\n        logprobs: Optional[int] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        seed: Optional[int] = None,\n        audio_timestamp: Optional[bool] = None,\n        media_resolution: Optional[MediaResolution] = None,\n        speech_config: Optional[SpeechConfig] = None,\n        enable_affective_dialog: Optional[bool] = None,\n    ) -&gt; None:\n        \"\"\"Initialize GenerationConfig with optional parameters.\n\n        Args:\n            stop_sequences (Optional[List[str]]):\n                List of strings that will stop generation when encountered\n            response_mime_type (Optional[str]):\n                MIME type for the response format\n            response_modalities (Optional[List[Modality]]):\n                List of modalities to include in the response\n            thinking_config (Optional[ThinkingConfig]):\n                Configuration for reasoning/thinking capabilities\n            temperature (Optional[float]):\n                Controls randomness in generation (0.0-1.0)\n            top_p (Optional[float]):\n                Nucleus sampling parameter (0.0-1.0)\n            top_k (Optional[int]):\n                Top-k sampling parameter\n            candidate_count (Optional[int]):\n                Number of response candidates to generate\n            max_output_tokens (Optional[int]):\n                Maximum number of tokens to generate\n            response_logprobs (Optional[bool]):\n                Whether to return log probabilities\n            logprobs (Optional[int]):\n                Number of log probabilities to return per token\n            presence_penalty (Optional[float]):\n                Penalty for token presence (-2.0 to 2.0)\n            frequency_penalty (Optional[float]):\n                Penalty for token frequency (-2.0 to 2.0)\n            seed (Optional[int]):\n                Random seed for deterministic generation\n            audio_timestamp (Optional[bool]):\n                Whether to include timestamps in audio responses\n            media_resolution (Optional[MediaResolution]):\n                Resolution setting for media content\n            speech_config (Optional[SpeechConfig]):\n                Configuration for speech synthesis\n            enable_affective_dialog (Optional[bool]):\n                Whether to enable emotional dialog features\n        \"\"\"\n        ...\n\n    def __str__(self) -&gt; str: ...\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.GenerationConfig.__init__","title":"<code>__init__(stop_sequences=None, response_mime_type=None, response_modalities=None, thinking_config=None, temperature=None, top_p=None, top_k=None, candidate_count=None, max_output_tokens=None, response_logprobs=None, logprobs=None, presence_penalty=None, frequency_penalty=None, seed=None, audio_timestamp=None, media_resolution=None, speech_config=None, enable_affective_dialog=None)</code>","text":"<p>Initialize GenerationConfig with optional parameters.</p> <p>Parameters:</p> Name Type Description Default <code>stop_sequences</code> <code>Optional[List[str]]</code> <p>List of strings that will stop generation when encountered</p> <code>None</code> <code>response_mime_type</code> <code>Optional[str]</code> <p>MIME type for the response format</p> <code>None</code> <code>response_modalities</code> <code>Optional[List[Modality]]</code> <p>List of modalities to include in the response</p> <code>None</code> <code>thinking_config</code> <code>Optional[ThinkingConfig]</code> <p>Configuration for reasoning/thinking capabilities</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>Controls randomness in generation (0.0-1.0)</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Nucleus sampling parameter (0.0-1.0)</p> <code>None</code> <code>top_k</code> <code>Optional[int]</code> <p>Top-k sampling parameter</p> <code>None</code> <code>candidate_count</code> <code>Optional[int]</code> <p>Number of response candidates to generate</p> <code>None</code> <code>max_output_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>response_logprobs</code> <code>Optional[bool]</code> <p>Whether to return log probabilities</p> <code>None</code> <code>logprobs</code> <code>Optional[int]</code> <p>Number of log probabilities to return per token</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Penalty for token presence (-2.0 to 2.0)</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Penalty for token frequency (-2.0 to 2.0)</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for deterministic generation</p> <code>None</code> <code>audio_timestamp</code> <code>Optional[bool]</code> <p>Whether to include timestamps in audio responses</p> <code>None</code> <code>media_resolution</code> <code>Optional[MediaResolution]</code> <p>Resolution setting for media content</p> <code>None</code> <code>speech_config</code> <code>Optional[SpeechConfig]</code> <p>Configuration for speech synthesis</p> <code>None</code> <code>enable_affective_dialog</code> <code>Optional[bool]</code> <p>Whether to enable emotional dialog features</p> <code>None</code> Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>def __init__(\n    self,\n    stop_sequences: Optional[List[str]] = None,\n    response_mime_type: Optional[str] = None,\n    response_modalities: Optional[List[Modality]] = None,\n    thinking_config: Optional[ThinkingConfig] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    top_k: Optional[int] = None,\n    candidate_count: Optional[int] = None,\n    max_output_tokens: Optional[int] = None,\n    response_logprobs: Optional[bool] = None,\n    logprobs: Optional[int] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    seed: Optional[int] = None,\n    audio_timestamp: Optional[bool] = None,\n    media_resolution: Optional[MediaResolution] = None,\n    speech_config: Optional[SpeechConfig] = None,\n    enable_affective_dialog: Optional[bool] = None,\n) -&gt; None:\n    \"\"\"Initialize GenerationConfig with optional parameters.\n\n    Args:\n        stop_sequences (Optional[List[str]]):\n            List of strings that will stop generation when encountered\n        response_mime_type (Optional[str]):\n            MIME type for the response format\n        response_modalities (Optional[List[Modality]]):\n            List of modalities to include in the response\n        thinking_config (Optional[ThinkingConfig]):\n            Configuration for reasoning/thinking capabilities\n        temperature (Optional[float]):\n            Controls randomness in generation (0.0-1.0)\n        top_p (Optional[float]):\n            Nucleus sampling parameter (0.0-1.0)\n        top_k (Optional[int]):\n            Top-k sampling parameter\n        candidate_count (Optional[int]):\n            Number of response candidates to generate\n        max_output_tokens (Optional[int]):\n            Maximum number of tokens to generate\n        response_logprobs (Optional[bool]):\n            Whether to return log probabilities\n        logprobs (Optional[int]):\n            Number of log probabilities to return per token\n        presence_penalty (Optional[float]):\n            Penalty for token presence (-2.0 to 2.0)\n        frequency_penalty (Optional[float]):\n            Penalty for token frequency (-2.0 to 2.0)\n        seed (Optional[int]):\n            Random seed for deterministic generation\n        audio_timestamp (Optional[bool]):\n            Whether to include timestamps in audio responses\n        media_resolution (Optional[MediaResolution]):\n            Resolution setting for media content\n        speech_config (Optional[SpeechConfig]):\n            Configuration for speech synthesis\n        enable_affective_dialog (Optional[bool]):\n            Whether to enable emotional dialog features\n    \"\"\"\n    ...\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.LatLng","title":"<code>LatLng</code>","text":"Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>class LatLng:\n    @property\n    def latitude(self) -&gt; float: ...\n    @property\n    def longitude(self) -&gt; float: ...\n    def __init__(self, latitude: float, longitude: float) -&gt; None:\n        \"\"\"Initialize LatLng with latitude and longitude.\n\n        Args:\n            latitude (float):\n                The latitude value.\n            longitude (float):\n                The longitude value.\n        \"\"\"\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.LatLng.__init__","title":"<code>__init__(latitude, longitude)</code>","text":"<p>Initialize LatLng with latitude and longitude.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>The latitude value.</p> required <code>longitude</code> <code>float</code> <p>The longitude value.</p> required Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>def __init__(self, latitude: float, longitude: float) -&gt; None:\n    \"\"\"Initialize LatLng with latitude and longitude.\n\n    Args:\n        latitude (float):\n            The latitude value.\n        longitude (float):\n            The longitude value.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.MediaResolution","title":"<code>MediaResolution</code>","text":"<p>Media resolution settings for content generation.</p> Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>class MediaResolution:\n    \"\"\"Media resolution settings for content generation.\"\"\"\n\n    MediaResolutionUnspecified: \"MediaResolution\"\n    MediaResolutionLow: \"MediaResolution\"\n    MediaResolutionMedium: \"MediaResolution\"\n    MediaResolutionHigh: \"MediaResolution\"\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.Modality","title":"<code>Modality</code>","text":"<p>Represents different modalities for content generation.</p> Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>class Modality:\n    \"\"\"Represents different modalities for content generation.\"\"\"\n\n    ModalityUnspecified: \"Modality\"\n    Text: \"Modality\"\n    Image: \"Modality\"\n    Audio: \"Modality\"\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.ModelArmorConfig","title":"<code>ModelArmorConfig</code>","text":"Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>class ModelArmorConfig:\n    def __init__(\n        self,\n        prompt_template_name: Optional[str],\n        response_template_name: Optional[str],\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            prompt_template_name (Optional[str]):\n                The name of the prompt template to use.\n            response_template_name (Optional[str]):\n                The name of the response template to use.\n        \"\"\"\n\n    @property\n    def prompt_template_name(self) -&gt; Optional[str]: ...\n    @property\n    def response_template_name(self) -&gt; Optional[str]: ...\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.ModelArmorConfig.__init__","title":"<code>__init__(prompt_template_name, response_template_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>prompt_template_name</code> <code>Optional[str]</code> <p>The name of the prompt template to use.</p> required <code>response_template_name</code> <code>Optional[str]</code> <p>The name of the response template to use.</p> required Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>def __init__(\n    self,\n    prompt_template_name: Optional[str],\n    response_template_name: Optional[str],\n) -&gt; None:\n    \"\"\"\n    Args:\n        prompt_template_name (Optional[str]):\n            The name of the prompt template to use.\n        response_template_name (Optional[str]):\n            The name of the response template to use.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.PrebuiltVoiceConfig","title":"<code>PrebuiltVoiceConfig</code>","text":"<p>Configuration for prebuilt voice models.</p> Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>class PrebuiltVoiceConfig:\n    \"\"\"Configuration for prebuilt voice models.\"\"\"\n\n    def __init__(\n        self,\n        voice_name: str,\n    ) -&gt; None: ...\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.PredictRequest","title":"<code>PredictRequest</code>","text":"Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>class PredictRequest:\n    def __init__(self, instances: List[dict], parameters: Optional[dict] = None) -&gt; None:\n        \"\"\"Request to pass to the Vertex Predict API when creating a request\n\n        Args:\n            instances (List[dict]):\n                A list of instances to be sent in the request.\n            parameters (Optional[dict]):\n                Optional parameters for the request.\n        \"\"\"\n\n    @property\n    def instances(self) -&gt; List[dict]: ...\n    @property\n    def parameters(self) -&gt; dict: ...\n    def __str__(self): ...\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.PredictRequest.__init__","title":"<code>__init__(instances, parameters=None)</code>","text":"<p>Request to pass to the Vertex Predict API when creating a request</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>List[dict]</code> <p>A list of instances to be sent in the request.</p> required <code>parameters</code> <code>Optional[dict]</code> <p>Optional parameters for the request.</p> <code>None</code> Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>def __init__(self, instances: List[dict], parameters: Optional[dict] = None) -&gt; None:\n    \"\"\"Request to pass to the Vertex Predict API when creating a request\n\n    Args:\n        instances (List[dict]):\n            A list of instances to be sent in the request.\n        parameters (Optional[dict]):\n            Optional parameters for the request.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.RetrievalConfig","title":"<code>RetrievalConfig</code>","text":"Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>class RetrievalConfig:\n    @property\n    def lat_lng(self) -&gt; LatLng: ...\n    @property\n    def language_code(self) -&gt; str: ...\n    def __init__(self, lat_lng: LatLng, language_code: str) -&gt; None:\n        \"\"\"Initialize RetrievalConfig with latitude/longitude and language code.\n\n        Args:\n            lat_lng (LatLng):\n                The latitude and longitude configuration.\n            language_code (str):\n                The language code for the retrieval.\n        \"\"\"\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.RetrievalConfig.__init__","title":"<code>__init__(lat_lng, language_code)</code>","text":"<p>Initialize RetrievalConfig with latitude/longitude and language code.</p> <p>Parameters:</p> Name Type Description Default <code>lat_lng</code> <code>LatLng</code> <p>The latitude and longitude configuration.</p> required <code>language_code</code> <code>str</code> <p>The language code for the retrieval.</p> required Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>def __init__(self, lat_lng: LatLng, language_code: str) -&gt; None:\n    \"\"\"Initialize RetrievalConfig with latitude/longitude and language code.\n\n    Args:\n        lat_lng (LatLng):\n            The latitude and longitude configuration.\n        language_code (str):\n            The language code for the retrieval.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.SafetySetting","title":"<code>SafetySetting</code>","text":"Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>class SafetySetting:\n    category: HarmCategory\n    threshold: HarmBlockThreshold\n    method: Optional[HarmBlockMethod]\n\n    def __init__(\n        self,\n        category: HarmCategory,\n        threshold: HarmBlockThreshold,\n        method: Optional[HarmBlockMethod] = None,\n    ) -&gt; None:\n        \"\"\"Initialize SafetySetting with required and optional parameters.\n\n        Args:\n            category (HarmCategory):\n                The category of harm to protect against.\n            threshold (HarmBlockThreshold):\n                The threshold for blocking content.\n            method (Optional[HarmBlockMethod]):\n                The method used for blocking (if any).\n        \"\"\"\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.SafetySetting.__init__","title":"<code>__init__(category, threshold, method=None)</code>","text":"<p>Initialize SafetySetting with required and optional parameters.</p> <p>Parameters:</p> Name Type Description Default <code>category</code> <code>HarmCategory</code> <p>The category of harm to protect against.</p> required <code>threshold</code> <code>HarmBlockThreshold</code> <p>The threshold for blocking content.</p> required <code>method</code> <code>Optional[HarmBlockMethod]</code> <p>The method used for blocking (if any).</p> <code>None</code> Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>def __init__(\n    self,\n    category: HarmCategory,\n    threshold: HarmBlockThreshold,\n    method: Optional[HarmBlockMethod] = None,\n) -&gt; None:\n    \"\"\"Initialize SafetySetting with required and optional parameters.\n\n    Args:\n        category (HarmCategory):\n            The category of harm to protect against.\n        threshold (HarmBlockThreshold):\n            The threshold for blocking content.\n        method (Optional[HarmBlockMethod]):\n            The method used for blocking (if any).\n    \"\"\"\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.SpeechConfig","title":"<code>SpeechConfig</code>","text":"<p>Configuration for speech generation.</p> Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>class SpeechConfig:\n    \"\"\"Configuration for speech generation.\"\"\"\n\n    def __init__(\n        self,\n        voice_config: Optional[\"VoiceConfig\"] = None,\n        language_code: Optional[str] = None,\n    ) -&gt; None: ...\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.ThinkingConfig","title":"<code>ThinkingConfig</code>","text":"<p>Configuration for thinking/reasoning capabilities.</p> Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>class ThinkingConfig:\n    \"\"\"Configuration for thinking/reasoning capabilities.\"\"\"\n\n    def __init__(\n        self,\n        include_thoughts: Optional[bool] = None,\n        thinking_budget: Optional[int] = None,\n    ) -&gt; None: ...\n</code></pre>"},{"location":"docs/api/google/#potato_head.google._google.VoiceConfig","title":"<code>VoiceConfig</code>","text":"<p>Configuration for voice generation.</p> Source code in <code>python/potato_head/google/_google.pyi</code> <pre><code>class VoiceConfig:\n    \"\"\"Configuration for voice generation.\"\"\"\n\n    def __init__(self, voice_config: VoiceConfigMode) -&gt; None: ...\n</code></pre>"},{"location":"docs/api/logging/","title":"Logging","text":""},{"location":"docs/api/logging/#potato_head.logging._logging.LoggingConfig","title":"<code>LoggingConfig</code>","text":"Source code in <code>python/potato_head/logging/_logging.pyi</code> <pre><code>class LoggingConfig:\n    show_threads: bool\n    log_level: LogLevel\n    write_level: WriteLevel\n    use_json: bool\n\n    def __init__(\n        self,\n        show_threads: bool = True,\n        log_level: LogLevel = LogLevel.Info,\n        write_level: WriteLevel = WriteLevel.Stdout,\n        use_json: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Logging configuration options.\n\n        Args:\n            show_threads:\n                Whether to include thread information in log messages.\n                Default is True.\n\n            log_level:\n                Log level for the logger.\n                Default is LogLevel.Info.\n\n            write_level:\n                Write level for the logger.\n                Default is WriteLevel.Stdout.\n\n            use_json:\n                Whether to write log messages in JSON format.\n                Default is False.\n        \"\"\"\n\n    @staticmethod\n    def json_default() -&gt; \"LoggingConfig\":\n        \"\"\"Gets a default JSON configuration.\n\n        show_threads: True\n        log_level: Env or LogLevel.Info\n        write_level: WriteLevel.Stdout\n        use_json: True\n\n        Returns:\n            LoggingConfig:\n                The default JSON configuration.\n        \"\"\"\n\n    @staticmethod\n    def default() -&gt; \"LoggingConfig\":\n        \"\"\"Gets a default configuration.\n\n        show_threads: True\n        log_level: Env or LogLevel.Info\n        write_level: WriteLevel.Stdout\n        use_json: False\n\n        Returns:\n            LoggingConfig:\n                The default JSON configuration.\n        \"\"\"\n</code></pre>"},{"location":"docs/api/logging/#potato_head.logging._logging.LoggingConfig.__init__","title":"<code>__init__(show_threads=True, log_level=LogLevel.Info, write_level=WriteLevel.Stdout, use_json=False)</code>","text":"<p>Logging configuration options.</p> <p>Parameters:</p> Name Type Description Default <code>show_threads</code> <code>bool</code> <p>Whether to include thread information in log messages. Default is True.</p> <code>True</code> <code>log_level</code> <code>LogLevel</code> <p>Log level for the logger. Default is LogLevel.Info.</p> <code>Info</code> <code>write_level</code> <code>WriteLevel</code> <p>Write level for the logger. Default is WriteLevel.Stdout.</p> <code>Stdout</code> <code>use_json</code> <code>bool</code> <p>Whether to write log messages in JSON format. Default is False.</p> <code>False</code> Source code in <code>python/potato_head/logging/_logging.pyi</code> <pre><code>def __init__(\n    self,\n    show_threads: bool = True,\n    log_level: LogLevel = LogLevel.Info,\n    write_level: WriteLevel = WriteLevel.Stdout,\n    use_json: bool = False,\n) -&gt; None:\n    \"\"\"\n    Logging configuration options.\n\n    Args:\n        show_threads:\n            Whether to include thread information in log messages.\n            Default is True.\n\n        log_level:\n            Log level for the logger.\n            Default is LogLevel.Info.\n\n        write_level:\n            Write level for the logger.\n            Default is WriteLevel.Stdout.\n\n        use_json:\n            Whether to write log messages in JSON format.\n            Default is False.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/logging/#potato_head.logging._logging.LoggingConfig.default","title":"<code>default()</code>  <code>staticmethod</code>","text":"<p>Gets a default configuration.</p> <p>show_threads: True log_level: Env or LogLevel.Info write_level: WriteLevel.Stdout use_json: False</p> <p>Returns:</p> Name Type Description <code>LoggingConfig</code> <code>LoggingConfig</code> <p>The default JSON configuration.</p> Source code in <code>python/potato_head/logging/_logging.pyi</code> <pre><code>@staticmethod\ndef default() -&gt; \"LoggingConfig\":\n    \"\"\"Gets a default configuration.\n\n    show_threads: True\n    log_level: Env or LogLevel.Info\n    write_level: WriteLevel.Stdout\n    use_json: False\n\n    Returns:\n        LoggingConfig:\n            The default JSON configuration.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/logging/#potato_head.logging._logging.LoggingConfig.json_default","title":"<code>json_default()</code>  <code>staticmethod</code>","text":"<p>Gets a default JSON configuration.</p> <p>show_threads: True log_level: Env or LogLevel.Info write_level: WriteLevel.Stdout use_json: True</p> <p>Returns:</p> Name Type Description <code>LoggingConfig</code> <code>LoggingConfig</code> <p>The default JSON configuration.</p> Source code in <code>python/potato_head/logging/_logging.pyi</code> <pre><code>@staticmethod\ndef json_default() -&gt; \"LoggingConfig\":\n    \"\"\"Gets a default JSON configuration.\n\n    show_threads: True\n    log_level: Env or LogLevel.Info\n    write_level: WriteLevel.Stdout\n    use_json: True\n\n    Returns:\n        LoggingConfig:\n            The default JSON configuration.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/logging/#potato_head.logging._logging.RustyLogger","title":"<code>RustyLogger</code>","text":"<p>The Rusty Logger class to use with your python and rust-backed projects.</p> Source code in <code>python/potato_head/logging/_logging.pyi</code> <pre><code>class RustyLogger:\n    \"\"\"The Rusty Logger class to use with your python and rust-backed projects.\"\"\"\n\n    @staticmethod\n    def setup_logging(config: Optional[LoggingConfig] = None) -&gt; None:\n        \"\"\"Sets up the logger with the given configuration.\n\n        Args:\n            config (LoggingConfig):\n                The configuration to use for the logger.\n        \"\"\"\n\n    @staticmethod\n    def get_logger(config: Optional[LoggingConfig] = None) -&gt; \"RustyLogger\":\n        \"\"\"Gets the logger instance.\n\n        Args:\n            config (LoggingConfig):\n                The configuration to use for the logger.\n\n        Returns:\n            RustyLogger:\n                The logger instance.\n        \"\"\"\n\n    def debug(self, message: str, *args) -&gt; None:\n        \"\"\"Logs a debug message.\n\n        Args:\n            message (str):\n                The message to log.\n\n            *args:\n                Additional arguments to log.\n        \"\"\"\n\n    def info(self, message: str, *args) -&gt; None:\n        \"\"\"Logs an info message.\n\n        Args:\n            message (str):\n                The message to log.\n\n            *args:\n                Additional arguments to log.\n        \"\"\"\n\n    def warn(self, message: str, *args) -&gt; None:\n        \"\"\"Logs a warning message.\n\n        Args:\n            message (str):\n                The message to log.\n\n            *args:\n                Additional arguments to log.\n        \"\"\"\n\n    def error(self, message: str, *args) -&gt; None:\n        \"\"\"Logs an error message.\n\n        Args:\n            message (str):\n                The message to log.\n\n            *args:\n                Additional arguments to log.\n        \"\"\"\n\n    def trace(self, message: str, *args) -&gt; None:\n        \"\"\"Logs a trace message.\n\n        Args:\n            message (str):\n                The message to log.\n\n            *args:\n                Additional arguments to log.\n        \"\"\"\n</code></pre>"},{"location":"docs/api/logging/#potato_head.logging._logging.RustyLogger.debug","title":"<code>debug(message, *args)</code>","text":"<p>Logs a debug message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required <code>*args</code> <p>Additional arguments to log.</p> <code>()</code> Source code in <code>python/potato_head/logging/_logging.pyi</code> <pre><code>def debug(self, message: str, *args) -&gt; None:\n    \"\"\"Logs a debug message.\n\n    Args:\n        message (str):\n            The message to log.\n\n        *args:\n            Additional arguments to log.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/logging/#potato_head.logging._logging.RustyLogger.error","title":"<code>error(message, *args)</code>","text":"<p>Logs an error message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required <code>*args</code> <p>Additional arguments to log.</p> <code>()</code> Source code in <code>python/potato_head/logging/_logging.pyi</code> <pre><code>def error(self, message: str, *args) -&gt; None:\n    \"\"\"Logs an error message.\n\n    Args:\n        message (str):\n            The message to log.\n\n        *args:\n            Additional arguments to log.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/logging/#potato_head.logging._logging.RustyLogger.get_logger","title":"<code>get_logger(config=None)</code>  <code>staticmethod</code>","text":"<p>Gets the logger instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>LoggingConfig</code> <p>The configuration to use for the logger.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>RustyLogger</code> <code>RustyLogger</code> <p>The logger instance.</p> Source code in <code>python/potato_head/logging/_logging.pyi</code> <pre><code>@staticmethod\ndef get_logger(config: Optional[LoggingConfig] = None) -&gt; \"RustyLogger\":\n    \"\"\"Gets the logger instance.\n\n    Args:\n        config (LoggingConfig):\n            The configuration to use for the logger.\n\n    Returns:\n        RustyLogger:\n            The logger instance.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/logging/#potato_head.logging._logging.RustyLogger.info","title":"<code>info(message, *args)</code>","text":"<p>Logs an info message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required <code>*args</code> <p>Additional arguments to log.</p> <code>()</code> Source code in <code>python/potato_head/logging/_logging.pyi</code> <pre><code>def info(self, message: str, *args) -&gt; None:\n    \"\"\"Logs an info message.\n\n    Args:\n        message (str):\n            The message to log.\n\n        *args:\n            Additional arguments to log.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/logging/#potato_head.logging._logging.RustyLogger.setup_logging","title":"<code>setup_logging(config=None)</code>  <code>staticmethod</code>","text":"<p>Sets up the logger with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>LoggingConfig</code> <p>The configuration to use for the logger.</p> <code>None</code> Source code in <code>python/potato_head/logging/_logging.pyi</code> <pre><code>@staticmethod\ndef setup_logging(config: Optional[LoggingConfig] = None) -&gt; None:\n    \"\"\"Sets up the logger with the given configuration.\n\n    Args:\n        config (LoggingConfig):\n            The configuration to use for the logger.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/logging/#potato_head.logging._logging.RustyLogger.trace","title":"<code>trace(message, *args)</code>","text":"<p>Logs a trace message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required <code>*args</code> <p>Additional arguments to log.</p> <code>()</code> Source code in <code>python/potato_head/logging/_logging.pyi</code> <pre><code>def trace(self, message: str, *args) -&gt; None:\n    \"\"\"Logs a trace message.\n\n    Args:\n        message (str):\n            The message to log.\n\n        *args:\n            Additional arguments to log.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/logging/#potato_head.logging._logging.RustyLogger.warn","title":"<code>warn(message, *args)</code>","text":"<p>Logs a warning message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required <code>*args</code> <p>Additional arguments to log.</p> <code>()</code> Source code in <code>python/potato_head/logging/_logging.pyi</code> <pre><code>def warn(self, message: str, *args) -&gt; None:\n    \"\"\"Logs a warning message.\n\n    Args:\n        message (str):\n            The message to log.\n\n        *args:\n            Additional arguments to log.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/mock/","title":"Mock","text":""},{"location":"docs/api/mock/#potato_head.mock._mock.LLMTestServer","title":"<code>LLMTestServer</code>","text":"<p>Mock server for OpenAI API. This class is used to simulate the OpenAI API for testing purposes.</p> Source code in <code>python/potato_head/mock/_mock.pyi</code> <pre><code>class LLMTestServer:\n    \"\"\"\n    Mock server for OpenAI API.\n    This class is used to simulate the OpenAI API for testing purposes.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def __enter__(self):\n        \"\"\"\n        Start the mock server.\n        \"\"\"\n        pass\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        \"\"\"\n        Stop the mock server.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"docs/api/mock/#potato_head.mock._mock.LLMTestServer.__enter__","title":"<code>__enter__()</code>","text":"<p>Start the mock server.</p> Source code in <code>python/potato_head/mock/_mock.pyi</code> <pre><code>def __enter__(self):\n    \"\"\"\n    Start the mock server.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"docs/api/mock/#potato_head.mock._mock.LLMTestServer.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"<p>Stop the mock server.</p> Source code in <code>python/potato_head/mock/_mock.pyi</code> <pre><code>def __exit__(self, exc_type, exc_value, traceback):\n    \"\"\"\n    Stop the mock server.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"docs/api/openai/","title":"OpenAI","text":""},{"location":"docs/api/openai/#potato_head.openai._openai.OpenAIChatSettings","title":"<code>OpenAIChatSettings</code>","text":"<p>OpenAI chat completion settings configuration.</p> <p>This class provides configuration options for OpenAI chat completions, including model parameters, tool usage, and request options.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; settings = OpenAIChatSettings(\n...     temperature=0.7,\n...     max_completion_tokens=1000,\n...     stream=True\n... )\n&gt;&gt;&gt; settings.temperature = 0.5\n</code></pre> Source code in <code>python/potato_head/openai/_openai.pyi</code> <pre><code>class OpenAIChatSettings:\n    \"\"\"OpenAI chat completion settings configuration.\n\n    This class provides configuration options for OpenAI chat completions,\n    including model parameters, tool usage, and request options.\n\n    Examples:\n        &gt;&gt;&gt; settings = OpenAIChatSettings(\n        ...     temperature=0.7,\n        ...     max_completion_tokens=1000,\n        ...     stream=True\n        ... )\n        &gt;&gt;&gt; settings.temperature = 0.5\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        max_completion_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        top_k: Optional[int] = None,\n        frequency_penalty: Optional[float] = None,\n        timeout: Optional[float] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        seed: Optional[int] = None,\n        logit_bias: Optional[Dict[str, int]] = None,\n        stop_sequences: Optional[List[str]] = None,\n        logprobs: Optional[bool] = None,\n        audio: Optional[AudioParam] = None,\n        metadata: Optional[Dict[str, str]] = None,\n        modalities: Optional[List[str]] = None,\n        n: Optional[int] = None,\n        prediction: Optional[Prediction] = None,\n        presence_penalty: Optional[float] = None,\n        prompt_cache_key: Optional[str] = None,\n        reasoning_effort: Optional[str] = None,\n        safety_identifier: Optional[str] = None,\n        service_tier: Optional[str] = None,\n        store: Optional[bool] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[StreamOptions] = None,\n        tool_choice: Optional[ToolChoice] = None,\n        tools: Optional[List[Tool]] = None,\n        top_logprobs: Optional[int] = None,\n        verbosity: Optional[str] = None,\n        extra_body: Optional[Any] = None,\n    ) -&gt; None:\n        \"\"\"Initialize OpenAI chat settings.\n\n        Args:\n            max_completion_tokens (Optional[int]):\n                Maximum number of tokens to generate\n            temperature (Optional[float]):\n                Sampling temperature (0.0 to 2.0)\n            top_p (Optional[float]):\n                Nucleus sampling parameter\n            top_k (Optional[int]):\n                Top-k sampling parameter\n            frequency_penalty (Optional[float]):\n                Frequency penalty (-2.0 to 2.0)\n            timeout (Optional[float]):\n                Request timeout in seconds\n            parallel_tool_calls (Optional[bool]):\n                Whether to enable parallel tool calls\n            seed (Optional[int]):\n                Random seed for deterministic outputs\n            logit_bias (Optional[Dict[str, int]]):\n                Token bias modifications\n            stop_sequences (Optional[List[str]]):\n                Sequences where generation should stop\n            logprobs (Optional[bool]):\n                Whether to return log probabilities\n            audio (Optional[AudioParam]):\n                Audio generation parameters\n            metadata (Optional[Dict[str, str]]):\n                Additional metadata for the request\n            modalities (Optional[List[str]]):\n                List of modalities to use\n            n (Optional[int]):\n                Number of completions to generate\n            prediction (Optional[Prediction]):\n                Prediction configuration\n            presence_penalty (Optional[float]):\n                Presence penalty (-2.0 to 2.0)\n            prompt_cache_key (Optional[str]):\n                Key for prompt caching\n            reasoning_effort (Optional[str]):\n                Reasoning effort level\n            safety_identifier (Optional[str]):\n                Safety configuration identifier\n            service_tier (Optional[str]):\n                Service tier to use\n            store (Optional[bool]):\n                Whether to store the conversation\n            stream (Optional[bool]):\n                Whether to stream the response\n            stream_options (Optional[StreamOptions]):\n                Streaming configuration options\n            tool_choice (Optional[ToolChoice]):\n                Tool choice configuration\n            tools (Optional[List[Tool]]):\n                Available tools for the model\n            top_logprobs (Optional[int]):\n                Number of top log probabilities to return\n            verbosity (Optional[str]):\n                Verbosity level for the response\n            extra_body (Optional[Any]):\n                Additional request body parameters\n        \"\"\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return string representation of the settings.\"\"\"\n</code></pre>"},{"location":"docs/api/openai/#potato_head.openai._openai.OpenAIChatSettings.__init__","title":"<code>__init__(*, max_completion_tokens=None, temperature=None, top_p=None, top_k=None, frequency_penalty=None, timeout=None, parallel_tool_calls=None, seed=None, logit_bias=None, stop_sequences=None, logprobs=None, audio=None, metadata=None, modalities=None, n=None, prediction=None, presence_penalty=None, prompt_cache_key=None, reasoning_effort=None, safety_identifier=None, service_tier=None, store=None, stream=None, stream_options=None, tool_choice=None, tools=None, top_logprobs=None, verbosity=None, extra_body=None)</code>","text":"<p>Initialize OpenAI chat settings.</p> <p>Parameters:</p> Name Type Description Default <code>max_completion_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>Sampling temperature (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Nucleus sampling parameter</p> <code>None</code> <code>top_k</code> <code>Optional[int]</code> <p>Top-k sampling parameter</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Frequency penalty (-2.0 to 2.0)</p> <code>None</code> <code>timeout</code> <code>Optional[float]</code> <p>Request timeout in seconds</p> <code>None</code> <code>parallel_tool_calls</code> <code>Optional[bool]</code> <p>Whether to enable parallel tool calls</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for deterministic outputs</p> <code>None</code> <code>logit_bias</code> <code>Optional[Dict[str, int]]</code> <p>Token bias modifications</p> <code>None</code> <code>stop_sequences</code> <code>Optional[List[str]]</code> <p>Sequences where generation should stop</p> <code>None</code> <code>logprobs</code> <code>Optional[bool]</code> <p>Whether to return log probabilities</p> <code>None</code> <code>audio</code> <code>Optional[AudioParam]</code> <p>Audio generation parameters</p> <code>None</code> <code>metadata</code> <code>Optional[Dict[str, str]]</code> <p>Additional metadata for the request</p> <code>None</code> <code>modalities</code> <code>Optional[List[str]]</code> <p>List of modalities to use</p> <code>None</code> <code>n</code> <code>Optional[int]</code> <p>Number of completions to generate</p> <code>None</code> <code>prediction</code> <code>Optional[Prediction]</code> <p>Prediction configuration</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Presence penalty (-2.0 to 2.0)</p> <code>None</code> <code>prompt_cache_key</code> <code>Optional[str]</code> <p>Key for prompt caching</p> <code>None</code> <code>reasoning_effort</code> <code>Optional[str]</code> <p>Reasoning effort level</p> <code>None</code> <code>safety_identifier</code> <code>Optional[str]</code> <p>Safety configuration identifier</p> <code>None</code> <code>service_tier</code> <code>Optional[str]</code> <p>Service tier to use</p> <code>None</code> <code>store</code> <code>Optional[bool]</code> <p>Whether to store the conversation</p> <code>None</code> <code>stream</code> <code>Optional[bool]</code> <p>Whether to stream the response</p> <code>None</code> <code>stream_options</code> <code>Optional[StreamOptions]</code> <p>Streaming configuration options</p> <code>None</code> <code>tool_choice</code> <code>Optional[ToolChoice]</code> <p>Tool choice configuration</p> <code>None</code> <code>tools</code> <code>Optional[List[Tool]]</code> <p>Available tools for the model</p> <code>None</code> <code>top_logprobs</code> <code>Optional[int]</code> <p>Number of top log probabilities to return</p> <code>None</code> <code>verbosity</code> <code>Optional[str]</code> <p>Verbosity level for the response</p> <code>None</code> <code>extra_body</code> <code>Optional[Any]</code> <p>Additional request body parameters</p> <code>None</code> Source code in <code>python/potato_head/openai/_openai.pyi</code> <pre><code>def __init__(\n    self,\n    *,\n    max_completion_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    top_k: Optional[int] = None,\n    frequency_penalty: Optional[float] = None,\n    timeout: Optional[float] = None,\n    parallel_tool_calls: Optional[bool] = None,\n    seed: Optional[int] = None,\n    logit_bias: Optional[Dict[str, int]] = None,\n    stop_sequences: Optional[List[str]] = None,\n    logprobs: Optional[bool] = None,\n    audio: Optional[AudioParam] = None,\n    metadata: Optional[Dict[str, str]] = None,\n    modalities: Optional[List[str]] = None,\n    n: Optional[int] = None,\n    prediction: Optional[Prediction] = None,\n    presence_penalty: Optional[float] = None,\n    prompt_cache_key: Optional[str] = None,\n    reasoning_effort: Optional[str] = None,\n    safety_identifier: Optional[str] = None,\n    service_tier: Optional[str] = None,\n    store: Optional[bool] = None,\n    stream: Optional[bool] = None,\n    stream_options: Optional[StreamOptions] = None,\n    tool_choice: Optional[ToolChoice] = None,\n    tools: Optional[List[Tool]] = None,\n    top_logprobs: Optional[int] = None,\n    verbosity: Optional[str] = None,\n    extra_body: Optional[Any] = None,\n) -&gt; None:\n    \"\"\"Initialize OpenAI chat settings.\n\n    Args:\n        max_completion_tokens (Optional[int]):\n            Maximum number of tokens to generate\n        temperature (Optional[float]):\n            Sampling temperature (0.0 to 2.0)\n        top_p (Optional[float]):\n            Nucleus sampling parameter\n        top_k (Optional[int]):\n            Top-k sampling parameter\n        frequency_penalty (Optional[float]):\n            Frequency penalty (-2.0 to 2.0)\n        timeout (Optional[float]):\n            Request timeout in seconds\n        parallel_tool_calls (Optional[bool]):\n            Whether to enable parallel tool calls\n        seed (Optional[int]):\n            Random seed for deterministic outputs\n        logit_bias (Optional[Dict[str, int]]):\n            Token bias modifications\n        stop_sequences (Optional[List[str]]):\n            Sequences where generation should stop\n        logprobs (Optional[bool]):\n            Whether to return log probabilities\n        audio (Optional[AudioParam]):\n            Audio generation parameters\n        metadata (Optional[Dict[str, str]]):\n            Additional metadata for the request\n        modalities (Optional[List[str]]):\n            List of modalities to use\n        n (Optional[int]):\n            Number of completions to generate\n        prediction (Optional[Prediction]):\n            Prediction configuration\n        presence_penalty (Optional[float]):\n            Presence penalty (-2.0 to 2.0)\n        prompt_cache_key (Optional[str]):\n            Key for prompt caching\n        reasoning_effort (Optional[str]):\n            Reasoning effort level\n        safety_identifier (Optional[str]):\n            Safety configuration identifier\n        service_tier (Optional[str]):\n            Service tier to use\n        store (Optional[bool]):\n            Whether to store the conversation\n        stream (Optional[bool]):\n            Whether to stream the response\n        stream_options (Optional[StreamOptions]):\n            Streaming configuration options\n        tool_choice (Optional[ToolChoice]):\n            Tool choice configuration\n        tools (Optional[List[Tool]]):\n            Available tools for the model\n        top_logprobs (Optional[int]):\n            Number of top log probabilities to return\n        verbosity (Optional[str]):\n            Verbosity level for the response\n        extra_body (Optional[Any]):\n            Additional request body parameters\n    \"\"\"\n</code></pre>"},{"location":"docs/api/openai/#potato_head.openai._openai.OpenAIChatSettings.__str__","title":"<code>__str__()</code>","text":"<p>Return string representation of the settings.</p> Source code in <code>python/potato_head/openai/_openai.pyi</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return string representation of the settings.\"\"\"\n</code></pre>"},{"location":"docs/api/openai/#potato_head.openai._openai.OpenAIEmbeddingConfig","title":"<code>OpenAIEmbeddingConfig</code>","text":"<p>OpenAI embedding configuration settings.</p> Source code in <code>python/potato_head/openai/_openai.pyi</code> <pre><code>class OpenAIEmbeddingConfig:\n    \"\"\"OpenAI embedding configuration settings.\"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        dimensions: Optional[int] = None,\n        encoding_format: Optional[str] = None,\n        user: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Initialize OpenAI embedding configuration.\n\n        Args:\n            model (str):\n                The embedding model to use.\n            dimensions (Optional[int]):\n                The output dimensionality of the embeddings.\n            encoding_format (Optional[str]):\n                The encoding format to use for the embeddings.\n                Can be either \"float\" or \"base64\".\n            user (Optional[str]):\n                The user ID for the embedding request.\n        \"\"\"\n\n    @property\n    def model(self) -&gt; str: ...\n    @property\n    def dimensions(self) -&gt; Optional[int]: ...\n    @property\n    def encoding_format(self) -&gt; Optional[str]: ...\n    @property\n    def user(self) -&gt; Optional[str]: ...\n</code></pre>"},{"location":"docs/api/openai/#potato_head.openai._openai.OpenAIEmbeddingConfig.__init__","title":"<code>__init__(model, dimensions=None, encoding_format=None, user=None)</code>","text":"<p>Initialize OpenAI embedding configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The embedding model to use.</p> required <code>dimensions</code> <code>Optional[int]</code> <p>The output dimensionality of the embeddings.</p> <code>None</code> <code>encoding_format</code> <code>Optional[str]</code> <p>The encoding format to use for the embeddings. Can be either \"float\" or \"base64\".</p> <code>None</code> <code>user</code> <code>Optional[str]</code> <p>The user ID for the embedding request.</p> <code>None</code> Source code in <code>python/potato_head/openai/_openai.pyi</code> <pre><code>def __init__(\n    self,\n    model: str,\n    dimensions: Optional[int] = None,\n    encoding_format: Optional[str] = None,\n    user: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Initialize OpenAI embedding configuration.\n\n    Args:\n        model (str):\n            The embedding model to use.\n        dimensions (Optional[int]):\n            The output dimensionality of the embeddings.\n        encoding_format (Optional[str]):\n            The encoding format to use for the embeddings.\n            Can be either \"float\" or \"base64\".\n        user (Optional[str]):\n            The user ID for the embedding request.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/","title":"PotatoHead","text":""},{"location":"docs/api/potato_head/#potato_head._potato_head.Agent","title":"<code>Agent</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class Agent:\n    def __init__(\n        self,\n        provider: Provider | str,\n        system_instruction: Optional[str | List[str] | Message | List[Message]] = None,\n    ) -&gt; None:\n        \"\"\"Create an Agent object.\n\n        Args:\n            provider (Provider | str):\n                The provider to use for the agent. This can be a Provider enum or a string\n                representing the provider.\n            system_instruction (Optional[str | List[str] | Message | List[Message]]):\n                The system message to use for the agent. This can be a string, a list of strings,\n                a Message object, or a list of Message objects. If None, no system message will be used.\n                This is added to all tasks that the agent executes. If a given task contains it's own\n                system message, the agent's system message will be prepended to the task's system message.\n\n        Example:\n        ```python\n            agent = Agent(\n                provider=Provider.OpenAI,\n                system_instruction=\"You are a helpful assistant.\",\n            )\n        ```\n        \"\"\"\n\n    @property\n    def system_instruction(self) -&gt; List[Message]:\n        \"\"\"The system message to use for the agent. This is a list of Message objects.\"\"\"\n\n    def execute_task(\n        self,\n        task: Task,\n        output_type: Optional[Any] = None,\n        model: Optional[str] = None,\n    ) -&gt; AgentResponse:\n        \"\"\"Execute a task.\n\n        Args:\n            task (Task):\n                The task to execute.\n            output_type (Optional[Any]):\n                The output type to use for the task. This can either be a Pydantic `BaseModel` class\n                or a supported PotatoHead response type such as `Score`.\n            model (Optional[str]):\n                The model to use for the task. If not provided, defaults to the `model` provided within\n                the Task's prompt. If the Task's prompt does not have a model, an error will be raised.\n\n        Returns:\n            AgentResponse:\n                The response from the agent after executing the task.\n        \"\"\"\n\n    def execute_prompt(\n        self,\n        prompt: Prompt,\n        output_type: Optional[Any] = None,\n        model: Optional[str] = None,\n    ) -&gt; AgentResponse:\n        \"\"\"Execute a prompt.\n\n        Args:\n            prompt (Prompt):`\n                The prompt to execute.\n            output_type (Optional[Any]):\n                The output type to use for the task. This can either be a Pydantic `BaseModel` class\n                or a supported potato_head response type such as `Score`.\n            model (Optional[str]):\n                The model to use for the task. If not provided, defaults to the `model` provided within\n                the Prompt. If the Prompt does not have a model, an error will be raised.\n\n        Returns:\n            AgentResponse:\n                The response from the agent after executing the task.\n        \"\"\"\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"The ID of the agent. This is a random uuid7 that is generated when the agent is created.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Agent.id","title":"<code>id</code>  <code>property</code>","text":"<p>The ID of the agent. This is a random uuid7 that is generated when the agent is created.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Agent.system_instruction","title":"<code>system_instruction</code>  <code>property</code>","text":"<p>The system message to use for the agent. This is a list of Message objects.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Agent.__init__","title":"<code>__init__(provider, system_instruction=None)</code>","text":"<p>Create an Agent object.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>Provider | str</code> <p>The provider to use for the agent. This can be a Provider enum or a string representing the provider.</p> required <code>system_instruction</code> <code>Optional[str | List[str] | Message | List[Message]]</code> <p>The system message to use for the agent. This can be a string, a list of strings, a Message object, or a list of Message objects. If None, no system message will be used. This is added to all tasks that the agent executes. If a given task contains it's own system message, the agent's system message will be prepended to the task's system message.</p> <code>None</code> <p>Example: <pre><code>    agent = Agent(\n        provider=Provider.OpenAI,\n        system_instruction=\"You are a helpful assistant.\",\n    )\n</code></pre></p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def __init__(\n    self,\n    provider: Provider | str,\n    system_instruction: Optional[str | List[str] | Message | List[Message]] = None,\n) -&gt; None:\n    \"\"\"Create an Agent object.\n\n    Args:\n        provider (Provider | str):\n            The provider to use for the agent. This can be a Provider enum or a string\n            representing the provider.\n        system_instruction (Optional[str | List[str] | Message | List[Message]]):\n            The system message to use for the agent. This can be a string, a list of strings,\n            a Message object, or a list of Message objects. If None, no system message will be used.\n            This is added to all tasks that the agent executes. If a given task contains it's own\n            system message, the agent's system message will be prepended to the task's system message.\n\n    Example:\n    ```python\n        agent = Agent(\n            provider=Provider.OpenAI,\n            system_instruction=\"You are a helpful assistant.\",\n        )\n    ```\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Agent.execute_prompt","title":"<code>execute_prompt(prompt, output_type=None, model=None)</code>","text":"<p>Execute a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Prompt</code> <p>` The prompt to execute.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type to use for the task. This can either be a Pydantic <code>BaseModel</code> class or a supported potato_head response type such as <code>Score</code>.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>The model to use for the task. If not provided, defaults to the <code>model</code> provided within the Prompt. If the Prompt does not have a model, an error will be raised.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>AgentResponse</code> <code>AgentResponse</code> <p>The response from the agent after executing the task.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def execute_prompt(\n    self,\n    prompt: Prompt,\n    output_type: Optional[Any] = None,\n    model: Optional[str] = None,\n) -&gt; AgentResponse:\n    \"\"\"Execute a prompt.\n\n    Args:\n        prompt (Prompt):`\n            The prompt to execute.\n        output_type (Optional[Any]):\n            The output type to use for the task. This can either be a Pydantic `BaseModel` class\n            or a supported potato_head response type such as `Score`.\n        model (Optional[str]):\n            The model to use for the task. If not provided, defaults to the `model` provided within\n            the Prompt. If the Prompt does not have a model, an error will be raised.\n\n    Returns:\n        AgentResponse:\n            The response from the agent after executing the task.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Agent.execute_task","title":"<code>execute_task(task, output_type=None, model=None)</code>","text":"<p>Execute a task.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>The task to execute.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type to use for the task. This can either be a Pydantic <code>BaseModel</code> class or a supported PotatoHead response type such as <code>Score</code>.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>The model to use for the task. If not provided, defaults to the <code>model</code> provided within the Task's prompt. If the Task's prompt does not have a model, an error will be raised.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>AgentResponse</code> <code>AgentResponse</code> <p>The response from the agent after executing the task.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def execute_task(\n    self,\n    task: Task,\n    output_type: Optional[Any] = None,\n    model: Optional[str] = None,\n) -&gt; AgentResponse:\n    \"\"\"Execute a task.\n\n    Args:\n        task (Task):\n            The task to execute.\n        output_type (Optional[Any]):\n            The output type to use for the task. This can either be a Pydantic `BaseModel` class\n            or a supported PotatoHead response type such as `Score`.\n        model (Optional[str]):\n            The model to use for the task. If not provided, defaults to the `model` provided within\n            the Task's prompt. If the Task's prompt does not have a model, an error will be raised.\n\n    Returns:\n        AgentResponse:\n            The response from the agent after executing the task.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.AgentResponse","title":"<code>AgentResponse</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class AgentResponse:\n    @property\n    def id(self) -&gt; str:\n        \"\"\"The ID of the agent response.\"\"\"\n\n    @property\n    def result(self) -&gt; Any:\n        \"\"\"The result of the agent response. This can be a Pydantic BaseModel class or a supported potato_head response type such as `Score`.\n        If neither is provided, the response json will be returned as a dictionary.\n        \"\"\"\n\n    @property\n    def token_usage(self) -&gt; Usage:\n        \"\"\"Returns the token usage of the agent response if supported\"\"\"\n\n    @property\n    def log_probs(self) -&gt; List[\"ResponseLogProbs\"]:\n        \"\"\"Returns the log probabilities of the agent response if supported.\n        This is primarily used for debugging and analysis purposes.\n        \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.AgentResponse.id","title":"<code>id</code>  <code>property</code>","text":"<p>The ID of the agent response.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.AgentResponse.log_probs","title":"<code>log_probs</code>  <code>property</code>","text":"<p>Returns the log probabilities of the agent response if supported. This is primarily used for debugging and analysis purposes.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.AgentResponse.result","title":"<code>result</code>  <code>property</code>","text":"<p>The result of the agent response. This can be a Pydantic BaseModel class or a supported potato_head response type such as <code>Score</code>. If neither is provided, the response json will be returned as a dictionary.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.AgentResponse.token_usage","title":"<code>token_usage</code>  <code>property</code>","text":"<p>Returns the token usage of the agent response if supported</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.AudioUrl","title":"<code>AudioUrl</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class AudioUrl:\n    def __init__(\n        self,\n        url: str,\n        kind: Literal[\"audio-url\"] = \"audio-url\",\n    ) -&gt; None:\n        \"\"\"Create an AudioUrl object.\n\n        Args:\n            url (str):\n                The URL of the audio.\n            kind (Literal[\"audio-url\"]):\n                The kind of the content.\n        \"\"\"\n\n    @property\n    def url(self) -&gt; str:\n        \"\"\"The URL of the audio.\"\"\"\n\n    @property\n    def kind(self) -&gt; str:\n        \"\"\"The kind of the content.\"\"\"\n\n    @property\n    def media_type(self) -&gt; str:\n        \"\"\"The media type of the audio URL.\"\"\"\n\n    @property\n    def format(self) -&gt; str:\n        \"\"\"The format of the audio URL.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.AudioUrl.format","title":"<code>format</code>  <code>property</code>","text":"<p>The format of the audio URL.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.AudioUrl.kind","title":"<code>kind</code>  <code>property</code>","text":"<p>The kind of the content.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.AudioUrl.media_type","title":"<code>media_type</code>  <code>property</code>","text":"<p>The media type of the audio URL.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.AudioUrl.url","title":"<code>url</code>  <code>property</code>","text":"<p>The URL of the audio.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.AudioUrl.__init__","title":"<code>__init__(url, kind='audio-url')</code>","text":"<p>Create an AudioUrl object.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the audio.</p> required <code>kind</code> <code>Literal['audio-url']</code> <p>The kind of the content.</p> <code>'audio-url'</code> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def __init__(\n    self,\n    url: str,\n    kind: Literal[\"audio-url\"] = \"audio-url\",\n) -&gt; None:\n    \"\"\"Create an AudioUrl object.\n\n    Args:\n        url (str):\n            The URL of the audio.\n        kind (Literal[\"audio-url\"]):\n            The kind of the content.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.BinaryContent","title":"<code>BinaryContent</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class BinaryContent:\n    def __init__(\n        self,\n        data: bytes,\n        media_type: str,\n        kind: str = \"binary\",\n    ) -&gt; None:\n        \"\"\"Create a BinaryContent object.\n\n        Args:\n            data (bytes):\n                The binary data.\n            media_type (str):\n                The media type of the binary data.\n            kind (str):\n                The kind of the content\n        \"\"\"\n\n    @property\n    def media_type(self) -&gt; str:\n        \"\"\"The media type of the binary content.\"\"\"\n\n    @property\n    def format(self) -&gt; str:\n        \"\"\"The format of the binary content.\"\"\"\n\n    @property\n    def data(self) -&gt; bytes:\n        \"\"\"The binary data.\"\"\"\n\n    @property\n    def kind(self) -&gt; str:\n        \"\"\"The kind of the content.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.BinaryContent.data","title":"<code>data</code>  <code>property</code>","text":"<p>The binary data.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.BinaryContent.format","title":"<code>format</code>  <code>property</code>","text":"<p>The format of the binary content.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.BinaryContent.kind","title":"<code>kind</code>  <code>property</code>","text":"<p>The kind of the content.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.BinaryContent.media_type","title":"<code>media_type</code>  <code>property</code>","text":"<p>The media type of the binary content.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.BinaryContent.__init__","title":"<code>__init__(data, media_type, kind='binary')</code>","text":"<p>Create a BinaryContent object.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The binary data.</p> required <code>media_type</code> <code>str</code> <p>The media type of the binary data.</p> required <code>kind</code> <code>str</code> <p>The kind of the content</p> <code>'binary'</code> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def __init__(\n    self,\n    data: bytes,\n    media_type: str,\n    kind: str = \"binary\",\n) -&gt; None:\n    \"\"\"Create a BinaryContent object.\n\n    Args:\n        data (bytes):\n            The binary data.\n        media_type (str):\n            The media type of the binary data.\n        kind (str):\n            The kind of the content\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.ChatResponse","title":"<code>ChatResponse</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class ChatResponse:\n    def to_py(self) -&gt; Any:\n        \"\"\"Convert the ChatResponse to it's Python representation.\"\"\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a string representation of the ChatResponse.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.ChatResponse.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of the ChatResponse.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a string representation of the ChatResponse.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.ChatResponse.to_py","title":"<code>to_py()</code>","text":"<p>Convert the ChatResponse to it's Python representation.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def to_py(self) -&gt; Any:\n    \"\"\"Convert the ChatResponse to it's Python representation.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.CompletionTokenDetails","title":"<code>CompletionTokenDetails</code>","text":"<p>Details about the completion tokens used in a model response.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class CompletionTokenDetails:\n    \"\"\"Details about the completion tokens used in a model response.\"\"\"\n\n    @property\n    def accepted_prediction_tokens(self) -&gt; int:\n        \"\"\"The number of accepted prediction tokens used in the response.\"\"\"\n\n    @property\n    def audio_tokens(self) -&gt; int:\n        \"\"\"The number of audio tokens used in the response.\"\"\"\n\n    @property\n    def reasoning_tokens(self) -&gt; int:\n        \"\"\"The number of reasoning tokens used in the response.\"\"\"\n\n    @property\n    def rejected_prediction_tokens(self) -&gt; int:\n        \"\"\"The number of rejected prediction tokens used in the response.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.CompletionTokenDetails.accepted_prediction_tokens","title":"<code>accepted_prediction_tokens</code>  <code>property</code>","text":"<p>The number of accepted prediction tokens used in the response.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.CompletionTokenDetails.audio_tokens","title":"<code>audio_tokens</code>  <code>property</code>","text":"<p>The number of audio tokens used in the response.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.CompletionTokenDetails.reasoning_tokens","title":"<code>reasoning_tokens</code>  <code>property</code>","text":"<p>The number of reasoning tokens used in the response.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.CompletionTokenDetails.rejected_prediction_tokens","title":"<code>rejected_prediction_tokens</code>  <code>property</code>","text":"<p>The number of rejected prediction tokens used in the response.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.DocumentUrl","title":"<code>DocumentUrl</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class DocumentUrl:\n    def __init__(\n        self,\n        url: str,\n        kind: Literal[\"document-url\"] = \"document-url\",\n    ) -&gt; None:\n        \"\"\"Create a DocumentUrl object.\n\n        Args:\n            url (str):\n                The URL of the document.\n            kind (Literal[\"document-url\"]):\n                The kind of the content.\n        \"\"\"\n\n    @property\n    def url(self) -&gt; str:\n        \"\"\"The URL of the document.\"\"\"\n\n    @property\n    def kind(self) -&gt; str:\n        \"\"\"The kind of the content.\"\"\"\n\n    @property\n    def media_type(self) -&gt; str:\n        \"\"\"The media type of the document URL.\"\"\"\n\n    @property\n    def format(self) -&gt; str:\n        \"\"\"The format of the document URL.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.DocumentUrl.format","title":"<code>format</code>  <code>property</code>","text":"<p>The format of the document URL.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.DocumentUrl.kind","title":"<code>kind</code>  <code>property</code>","text":"<p>The kind of the content.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.DocumentUrl.media_type","title":"<code>media_type</code>  <code>property</code>","text":"<p>The media type of the document URL.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.DocumentUrl.url","title":"<code>url</code>  <code>property</code>","text":"<p>The URL of the document.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.DocumentUrl.__init__","title":"<code>__init__(url, kind='document-url')</code>","text":"<p>Create a DocumentUrl object.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the document.</p> required <code>kind</code> <code>Literal['document-url']</code> <p>The kind of the content.</p> <code>'document-url'</code> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def __init__(\n    self,\n    url: str,\n    kind: Literal[\"document-url\"] = \"document-url\",\n) -&gt; None:\n    \"\"\"Create a DocumentUrl object.\n\n    Args:\n        url (str):\n            The URL of the document.\n        kind (Literal[\"document-url\"]):\n            The kind of the content.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Embedder","title":"<code>Embedder</code>","text":"<p>Class for creating embeddings.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class Embedder:\n    \"\"\"Class for creating embeddings.\"\"\"\n\n    def __init__(  # type: ignore\n        self,\n        provider: Provider | str,\n        config: Optional[OpenAIEmbeddingConfig | GeminiEmbeddingConfig] = None,\n    ) -&gt; None:\n        \"\"\"Create an Embedder object.\n\n        Args:\n            provider (Provider | str):\n                The provider to use for the embedder. This can be a Provider enum or a string\n                representing the provider.\n            config (Optional[OpenAIEmbeddingConfig | GeminiEmbeddingConfig]):\n                The configuration to use for the embedder. This can be a Pydantic BaseModel class\n                representing the configuration for the provider. If no config is provided,\n                defaults to OpenAI provider configuration.\n        \"\"\"\n\n    def embed(\n        self,\n        input: str | List[str] | PredictRequest,\n    ) -&gt; OpenAIEmbeddingResponse | GeminiEmbeddingResponse | PredictResponse:\n        \"\"\"Create embeddings for input.\n\n        Args:\n            input: The input to embed. Type depends on provider:\n                - OpenAI/Gemini: str | List[str]\n                - Vertex: PredictRequest\n\n        Returns:\n            Provider-specific response type.\n            OpenAIEmbeddingResponse for OpenAI,\n            GeminiEmbeddingResponse for Gemini,\n            PredictResponse for Vertex.\n\n        Examples:\n            ```python\n            ## OpenAI\n            embedder = Embedder(Provider.OpenAI)\n            response = embedder.embed(input=\"Test input\")\n\n            ## Gemini\n            embedder = Embedder(Provider.Gemini, config=GeminiEmbeddingConfig(model=\"gemini-embedding-001\"))\n            response = embedder.embed(input=\"Test input\")\n\n            ## Vertex\n            from potato_head.google import PredictRequest\n            embedder = Embedder(Provider.Vertex)\n            response = embedder.embed(input=PredictRequest(text=\"Test input\"))\n            ```\n        \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Embedder.__init__","title":"<code>__init__(provider, config=None)</code>","text":"<p>Create an Embedder object.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>Provider | str</code> <p>The provider to use for the embedder. This can be a Provider enum or a string representing the provider.</p> required <code>config</code> <code>Optional[OpenAIEmbeddingConfig | GeminiEmbeddingConfig]</code> <p>The configuration to use for the embedder. This can be a Pydantic BaseModel class representing the configuration for the provider. If no config is provided, defaults to OpenAI provider configuration.</p> <code>None</code> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def __init__(  # type: ignore\n    self,\n    provider: Provider | str,\n    config: Optional[OpenAIEmbeddingConfig | GeminiEmbeddingConfig] = None,\n) -&gt; None:\n    \"\"\"Create an Embedder object.\n\n    Args:\n        provider (Provider | str):\n            The provider to use for the embedder. This can be a Provider enum or a string\n            representing the provider.\n        config (Optional[OpenAIEmbeddingConfig | GeminiEmbeddingConfig]):\n            The configuration to use for the embedder. This can be a Pydantic BaseModel class\n            representing the configuration for the provider. If no config is provided,\n            defaults to OpenAI provider configuration.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Embedder.embed","title":"<code>embed(input)</code>","text":"<p>Create embeddings for input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str | List[str] | PredictRequest</code> <p>The input to embed. Type depends on provider: - OpenAI/Gemini: str | List[str] - Vertex: PredictRequest</p> required <p>Returns:</p> Type Description <code>OpenAIEmbeddingResponse | GeminiEmbeddingResponse | PredictResponse</code> <p>Provider-specific response type.</p> <code>OpenAIEmbeddingResponse | GeminiEmbeddingResponse | PredictResponse</code> <p>OpenAIEmbeddingResponse for OpenAI,</p> <code>OpenAIEmbeddingResponse | GeminiEmbeddingResponse | PredictResponse</code> <p>GeminiEmbeddingResponse for Gemini,</p> <code>OpenAIEmbeddingResponse | GeminiEmbeddingResponse | PredictResponse</code> <p>PredictResponse for Vertex.</p> <p>Examples:</p> <pre><code>## OpenAI\nembedder = Embedder(Provider.OpenAI)\nresponse = embedder.embed(input=\"Test input\")\n\n## Gemini\nembedder = Embedder(Provider.Gemini, config=GeminiEmbeddingConfig(model=\"gemini-embedding-001\"))\nresponse = embedder.embed(input=\"Test input\")\n\n## Vertex\nfrom potato_head.google import PredictRequest\nembedder = Embedder(Provider.Vertex)\nresponse = embedder.embed(input=PredictRequest(text=\"Test input\"))\n</code></pre> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def embed(\n    self,\n    input: str | List[str] | PredictRequest,\n) -&gt; OpenAIEmbeddingResponse | GeminiEmbeddingResponse | PredictResponse:\n    \"\"\"Create embeddings for input.\n\n    Args:\n        input: The input to embed. Type depends on provider:\n            - OpenAI/Gemini: str | List[str]\n            - Vertex: PredictRequest\n\n    Returns:\n        Provider-specific response type.\n        OpenAIEmbeddingResponse for OpenAI,\n        GeminiEmbeddingResponse for Gemini,\n        PredictResponse for Vertex.\n\n    Examples:\n        ```python\n        ## OpenAI\n        embedder = Embedder(Provider.OpenAI)\n        response = embedder.embed(input=\"Test input\")\n\n        ## Gemini\n        embedder = Embedder(Provider.Gemini, config=GeminiEmbeddingConfig(model=\"gemini-embedding-001\"))\n        response = embedder.embed(input=\"Test input\")\n\n        ## Vertex\n        from potato_head.google import PredictRequest\n        embedder = Embedder(Provider.Vertex)\n        response = embedder.embed(input=PredictRequest(text=\"Test input\"))\n        ```\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.EventDetails","title":"<code>EventDetails</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class EventDetails:\n    @property\n    def prompt(self) -&gt; Optional[Prompt]:\n        \"\"\"The prompt used for the task.\"\"\"\n\n    @property\n    def response(self) -&gt; Optional[ChatResponse]:\n        \"\"\"The response from the agent after executing the task.\"\"\"\n\n    @property\n    def duration(self) -&gt; Optional[datetime.timedelta]:\n        \"\"\"The duration of the task execution.\"\"\"\n\n    @property\n    def start_time(self) -&gt; Optional[datetime.datetime]:\n        \"\"\"The start time of the task execution.\"\"\"\n\n    @property\n    def end_time(self) -&gt; Optional[datetime.datetime]:\n        \"\"\"The end time of the task execution.\"\"\"\n\n    @property\n    def error(self) -&gt; Optional[str]:\n        \"\"\"The error message if the task failed, otherwise None.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.EventDetails.duration","title":"<code>duration</code>  <code>property</code>","text":"<p>The duration of the task execution.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.EventDetails.end_time","title":"<code>end_time</code>  <code>property</code>","text":"<p>The end time of the task execution.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.EventDetails.error","title":"<code>error</code>  <code>property</code>","text":"<p>The error message if the task failed, otherwise None.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.EventDetails.prompt","title":"<code>prompt</code>  <code>property</code>","text":"<p>The prompt used for the task.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.EventDetails.response","title":"<code>response</code>  <code>property</code>","text":"<p>The response from the agent after executing the task.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.EventDetails.start_time","title":"<code>start_time</code>  <code>property</code>","text":"<p>The start time of the task execution.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.ImageUrl","title":"<code>ImageUrl</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class ImageUrl:\n    def __init__(\n        self,\n        url: str,\n        kind: Literal[\"image-url\"] = \"image-url\",\n    ) -&gt; None:\n        \"\"\"Create an ImageUrl object.\n\n        Args:\n            url (str):\n                The URL of the image.\n            kind (Literal[\"image-url\"]):\n                The kind of the content.\n        \"\"\"\n\n    @property\n    def url(self) -&gt; str:\n        \"\"\"The URL of the image.\"\"\"\n\n    @property\n    def kind(self) -&gt; str:\n        \"\"\"The kind of the content.\"\"\"\n\n    @property\n    def media_type(self) -&gt; str:\n        \"\"\"The media type of the image URL.\"\"\"\n\n    @property\n    def format(self) -&gt; str:\n        \"\"\"The format of the image URL.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.ImageUrl.format","title":"<code>format</code>  <code>property</code>","text":"<p>The format of the image URL.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.ImageUrl.kind","title":"<code>kind</code>  <code>property</code>","text":"<p>The kind of the content.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.ImageUrl.media_type","title":"<code>media_type</code>  <code>property</code>","text":"<p>The media type of the image URL.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.ImageUrl.url","title":"<code>url</code>  <code>property</code>","text":"<p>The URL of the image.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.ImageUrl.__init__","title":"<code>__init__(url, kind='image-url')</code>","text":"<p>Create an ImageUrl object.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the image.</p> required <code>kind</code> <code>Literal['image-url']</code> <p>The kind of the content.</p> <code>'image-url'</code> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def __init__(\n    self,\n    url: str,\n    kind: Literal[\"image-url\"] = \"image-url\",\n) -&gt; None:\n    \"\"\"Create an ImageUrl object.\n\n    Args:\n        url (str):\n            The URL of the image.\n        kind (Literal[\"image-url\"]):\n            The kind of the content.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.LogProbs","title":"<code>LogProbs</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class LogProbs:\n    @property\n    def tokens(self) -&gt; List[ResponseLogProbs]:\n        \"\"\"The log probabilities of the tokens in the response.\n        This is primarily used for debugging and analysis purposes.\n        \"\"\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of the log probabilities.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.LogProbs.tokens","title":"<code>tokens</code>  <code>property</code>","text":"<p>The log probabilities of the tokens in the response. This is primarily used for debugging and analysis purposes.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.LogProbs.__str__","title":"<code>__str__()</code>","text":"<p>String representation of the log probabilities.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation of the log probabilities.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Message","title":"<code>Message</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class Message:\n    def __init__(self, content: str | ImageUrl | AudioUrl | BinaryContent | DocumentUrl) -&gt; None:\n        \"\"\"Create a Message object.\n\n        Args:\n            content (str | ImageUrl | AudioUrl | BinaryContent | DocumentUrl):\n                The content of the message.\n        \"\"\"\n\n    @property\n    def content(self) -&gt; str | ImageUrl | AudioUrl | BinaryContent | DocumentUrl:\n        \"\"\"The content of the message\"\"\"\n\n    def bind(self, name: str, value: str) -&gt; \"Message\":\n        \"\"\"Bind context to a specific variable in the prompt. This is an immutable operation meaning that it\n        will return a new Message object with the context bound.\n\n            Example with Prompt that contains two messages\n\n            ```python\n                prompt = Prompt(\n                    model=\"openai:gpt-4o\",\n                    message=[\n                        \"My prompt variable is ${variable}\",\n                        \"This is another message\",\n                    ],\n                    system_instruction=\"system_prompt\",\n                )\n                bounded_prompt = prompt.message[0].bind(\"variable\", \"hello world\").unwrap() # we bind \"hello world\" to \"variable\"\n            ```\n\n        Args:\n            name (str):\n                The name of the variable to bind.\n            value (str):\n                The value to bind the variable to.\n\n        Returns:\n            Message:\n                The message with the context bound.\n        \"\"\"\n\n    def bind_mut(self, name: str, value: str) -&gt; \"Message\":\n        \"\"\"Bind context to a specific variable in the prompt. This is a mutable operation meaning that it\n        will modify the current Message object.\n\n            Example with Prompt that contains two messages\n\n            ```python\n                prompt = Prompt(\n                    model=\"openai:gpt-4o\",\n                    message=[\n                        \"My prompt variable is ${variable}\",\n                        \"This is another message\",\n                    ],\n                    system_instruction=\"system_prompt\",\n                )\n                prompt.message[0].bind_mut(\"variable\", \"hello world\") # we bind \"hello world\" to \"variable\"\n            ```\n\n        Args:\n            name (str):\n                The name of the variable to bind.\n            value (str):\n                The value to bind the variable to.\n\n        Returns:\n            Message:\n                The message with the context bound.\n        \"\"\"\n\n    def unwrap(self) -&gt; Any:\n        \"\"\"Unwrap the message content.\n\n        Returns:\n            A serializable representation of the message content, which can be a string, list, or dict.\n        \"\"\"\n\n    def model_dump(self) -&gt; Dict[str, Any]:\n        \"\"\"Unwrap the message content and serialize it to a dictionary.\n\n        Returns:\n            Dict[str, Any]:\n                The message dictionary with keys \"content\" and \"role\".\n        \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Message.content","title":"<code>content</code>  <code>property</code>","text":"<p>The content of the message</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Message.__init__","title":"<code>__init__(content)</code>","text":"<p>Create a Message object.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str | ImageUrl | AudioUrl | BinaryContent | DocumentUrl</code> <p>The content of the message.</p> required Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def __init__(self, content: str | ImageUrl | AudioUrl | BinaryContent | DocumentUrl) -&gt; None:\n    \"\"\"Create a Message object.\n\n    Args:\n        content (str | ImageUrl | AudioUrl | BinaryContent | DocumentUrl):\n            The content of the message.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Message.bind","title":"<code>bind(name, value)</code>","text":"<p>Bind context to a specific variable in the prompt. This is an immutable operation meaning that it will return a new Message object with the context bound.</p> <pre><code>Example with Prompt that contains two messages\n\n```python\n    prompt = Prompt(\n        model=\"openai:gpt-4o\",\n        message=[\n            \"My prompt variable is ${variable}\",\n            \"This is another message\",\n        ],\n        system_instruction=\"system_prompt\",\n    )\n    bounded_prompt = prompt.message[0].bind(\"variable\", \"hello world\").unwrap() # we bind \"hello world\" to \"variable\"\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the variable to bind.</p> required <code>value</code> <code>str</code> <p>The value to bind the variable to.</p> required <p>Returns:</p> Name Type Description <code>Message</code> <code>Message</code> <p>The message with the context bound.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def bind(self, name: str, value: str) -&gt; \"Message\":\n    \"\"\"Bind context to a specific variable in the prompt. This is an immutable operation meaning that it\n    will return a new Message object with the context bound.\n\n        Example with Prompt that contains two messages\n\n        ```python\n            prompt = Prompt(\n                model=\"openai:gpt-4o\",\n                message=[\n                    \"My prompt variable is ${variable}\",\n                    \"This is another message\",\n                ],\n                system_instruction=\"system_prompt\",\n            )\n            bounded_prompt = prompt.message[0].bind(\"variable\", \"hello world\").unwrap() # we bind \"hello world\" to \"variable\"\n        ```\n\n    Args:\n        name (str):\n            The name of the variable to bind.\n        value (str):\n            The value to bind the variable to.\n\n    Returns:\n        Message:\n            The message with the context bound.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Message.bind_mut","title":"<code>bind_mut(name, value)</code>","text":"<p>Bind context to a specific variable in the prompt. This is a mutable operation meaning that it will modify the current Message object.</p> <pre><code>Example with Prompt that contains two messages\n\n```python\n    prompt = Prompt(\n        model=\"openai:gpt-4o\",\n        message=[\n            \"My prompt variable is ${variable}\",\n            \"This is another message\",\n        ],\n        system_instruction=\"system_prompt\",\n    )\n    prompt.message[0].bind_mut(\"variable\", \"hello world\") # we bind \"hello world\" to \"variable\"\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the variable to bind.</p> required <code>value</code> <code>str</code> <p>The value to bind the variable to.</p> required <p>Returns:</p> Name Type Description <code>Message</code> <code>Message</code> <p>The message with the context bound.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def bind_mut(self, name: str, value: str) -&gt; \"Message\":\n    \"\"\"Bind context to a specific variable in the prompt. This is a mutable operation meaning that it\n    will modify the current Message object.\n\n        Example with Prompt that contains two messages\n\n        ```python\n            prompt = Prompt(\n                model=\"openai:gpt-4o\",\n                message=[\n                    \"My prompt variable is ${variable}\",\n                    \"This is another message\",\n                ],\n                system_instruction=\"system_prompt\",\n            )\n            prompt.message[0].bind_mut(\"variable\", \"hello world\") # we bind \"hello world\" to \"variable\"\n        ```\n\n    Args:\n        name (str):\n            The name of the variable to bind.\n        value (str):\n            The value to bind the variable to.\n\n    Returns:\n        Message:\n            The message with the context bound.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Message.model_dump","title":"<code>model_dump()</code>","text":"<p>Unwrap the message content and serialize it to a dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The message dictionary with keys \"content\" and \"role\".</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def model_dump(self) -&gt; Dict[str, Any]:\n    \"\"\"Unwrap the message content and serialize it to a dictionary.\n\n    Returns:\n        Dict[str, Any]:\n            The message dictionary with keys \"content\" and \"role\".\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Message.unwrap","title":"<code>unwrap()</code>","text":"<p>Unwrap the message content.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A serializable representation of the message content, which can be a string, list, or dict.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def unwrap(self) -&gt; Any:\n    \"\"\"Unwrap the message content.\n\n    Returns:\n        A serializable representation of the message content, which can be a string, list, or dict.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.ModelSettings","title":"<code>ModelSettings</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class ModelSettings:\n    def __init__(self, settings: OpenAIChatSettings | GeminiSettings) -&gt; None:\n        \"\"\"ModelSettings for configuring the model.\n\n        Args:\n            settings (OpenAIChatSettings | GeminiSettings):\n                The settings to use for the model. Currently supports OpenAI and Gemini settings.\n        \"\"\"\n\n    @property\n    def settings(self) -&gt; OpenAIChatSettings | GeminiSettings:\n        \"\"\"The settings to use for the model.\"\"\"\n\n    def model_dump_json(self) -&gt; str:\n        \"\"\"The JSON representation of the model settings.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.ModelSettings.settings","title":"<code>settings</code>  <code>property</code>","text":"<p>The settings to use for the model.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.ModelSettings.__init__","title":"<code>__init__(settings)</code>","text":"<p>ModelSettings for configuring the model.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>OpenAIChatSettings | GeminiSettings</code> <p>The settings to use for the model. Currently supports OpenAI and Gemini settings.</p> required Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def __init__(self, settings: OpenAIChatSettings | GeminiSettings) -&gt; None:\n    \"\"\"ModelSettings for configuring the model.\n\n    Args:\n        settings (OpenAIChatSettings | GeminiSettings):\n            The settings to use for the model. Currently supports OpenAI and Gemini settings.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.ModelSettings.model_dump_json","title":"<code>model_dump_json()</code>","text":"<p>The JSON representation of the model settings.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def model_dump_json(self) -&gt; str:\n    \"\"\"The JSON representation of the model settings.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Prompt","title":"<code>Prompt</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class Prompt:\n    def __init__(\n        self,\n        message: (\n            str\n            | Sequence[str | ImageUrl | AudioUrl | BinaryContent | DocumentUrl]\n            | Message\n            | List[Message]\n            | List[Dict[str, Any]]\n        ),\n        model: str,\n        provider: Provider | str,\n        system_instruction: Optional[str | List[str]] = None,\n        model_settings: Optional[ModelSettings | OpenAIChatSettings | GeminiSettings] = None,\n        response_format: Optional[Any] = None,\n    ) -&gt; None:\n        \"\"\"Prompt for interacting with an LLM API.\n\n        Args:\n            message (str | Sequence[str | ImageUrl | AudioUrl | BinaryContent | DocumentUrl] | Message | List[Message]):\n                The prompt to use.\n            model (str):\n                The model to use for the prompt\n            provider (Provider | str):\n                The provider to use for the prompt.\n            system_instruction (Optional[str | List[str]]):\n                The system prompt to use in the prompt.\n            model_settings (None):\n                The model settings to use for the prompt.\n                Defaults to None which means no model settings will be used\n            response_format (Optional[BaseModel | Score]):\n                The response format to use for the prompt. This is used for Structured Outputs\n                (https://platform.openai.com/docs/guides/structured-outputs?api-mode=chat).\n                Currently, response_format only support Pydantic BaseModel classes and the PotatoHead Score class.\n                The provided response_format will be parsed into a JSON schema.\n\n        \"\"\"\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"The model to use for the prompt.\"\"\"\n\n    @property\n    def provider(self) -&gt; str:\n        \"\"\"The provider to use for the prompt.\"\"\"\n\n    @property\n    def model_identifier(self) -&gt; Any:\n        \"\"\"Concatenation of provider and model, used for identifying the model in the prompt. This\n        is commonly used with pydantic_ai to identify the model to use for the agent.\n\n        Example:\n            ```python\n                prompt = Prompt(\n                    model=\"gpt-4o\",\n                    message=\"My prompt variable is ${variable}\",\n                    system_instruction=\"system_instruction\",\n                    provider=\"openai\",\n                )\n                agent = Agent(\n                    prompt.model_identifier, # \"openai:gpt-4o\"\n                    system_instructions=prompt.system_instruction[0].unwrap(),\n                )\n            ```\n        \"\"\"\n\n    @property\n    def model_settings(self) -&gt; ModelSettings:\n        \"\"\"The model settings to use for the prompt.\"\"\"\n\n    @property\n    def message(\n        self,\n    ) -&gt; List[Message]:\n        \"\"\"The user message to use in the prompt.\"\"\"\n\n    @property\n    def system_instruction(self) -&gt; List[Message]:\n        \"\"\"The system message to use in the prompt.\"\"\"\n\n    def save_prompt(self, path: Optional[Path] = None) -&gt; None:\n        \"\"\"Save the prompt to a file.\n\n        Args:\n            path (Optional[Path]):\n                The path to save the prompt to. If None, the prompt will be saved to\n                the current working directory.\n        \"\"\"\n\n    @staticmethod\n    def from_path(path: Path) -&gt; \"Prompt\":\n        \"\"\"Load a prompt from a file.\n\n        Args:\n            path (Path):\n                The path to the prompt file.\n\n        Returns:\n            Prompt:\n                The loaded prompt.\n        \"\"\"\n\n    @staticmethod\n    def model_validate_json(json_string: str) -&gt; \"Prompt\":\n        \"\"\"Validate the model JSON.\n\n        Args:\n            json_string (str):\n                The JSON string to validate.\n        Returns:\n            Prompt:\n                The prompt object.\n        \"\"\"\n\n    def model_dump_json(self) -&gt; str:\n        \"\"\"Dump the model to a JSON string.\n\n        Returns:\n            str:\n                The JSON string.\n        \"\"\"\n\n    def bind(\n        self,\n        name: Optional[str] = None,\n        value: Optional[str | int | float | bool | list] = None,\n        **kwargs: Any,\n    ) -&gt; \"Prompt\":\n        \"\"\"Bind context to a specific variable in the prompt. This is an immutable operation meaning that it\n        will return a new Prompt object with the context bound. This will iterate over all user messages.\n\n        Args:\n            name (str):\n                The name of the variable to bind.\n            value (str | int | float | bool | list):\n                The value to bind the variable to. Must be a JSON serializable type.\n            **kwargs (Any):\n                Additional keyword arguments to bind to the prompt. This can be used to bind multiple variables at once.\n\n        Returns:\n            Prompt:\n                The prompt with the context bound.\n        \"\"\"\n\n    def bind_mut(\n        self,\n        name: Optional[str] = None,\n        value: Optional[str | int | float | bool | list] = None,\n        **kwargs: Any,\n    ) -&gt; \"Prompt\":\n        \"\"\"Bind context to a specific variable in the prompt. This is a mutable operation meaning that it\n        will modify the current Prompt object. This will iterate over all user messages.\n\n        Args:\n            name (str):\n                The name of the variable to bind.\n            value (str | int | float | bool | list):\n                The value to bind the variable to. Must be a JSON serializable type.\n            **kwargs (Any):\n                Additional keyword arguments to bind to the prompt. This can be used to bind multiple variables at once.\n\n        Returns:\n            Prompt:\n                The prompt with the context bound.\n        \"\"\"\n\n    @property\n    def response_json_schema(self) -&gt; Optional[str]:\n        \"\"\"The JSON schema for the response if provided.\"\"\"\n\n    def __str__(self): ...\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Prompt.message","title":"<code>message</code>  <code>property</code>","text":"<p>The user message to use in the prompt.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Prompt.model","title":"<code>model</code>  <code>property</code>","text":"<p>The model to use for the prompt.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Prompt.model_identifier","title":"<code>model_identifier</code>  <code>property</code>","text":"<p>Concatenation of provider and model, used for identifying the model in the prompt. This is commonly used with pydantic_ai to identify the model to use for the agent.</p> Example <pre><code>    prompt = Prompt(\n        model=\"gpt-4o\",\n        message=\"My prompt variable is ${variable}\",\n        system_instruction=\"system_instruction\",\n        provider=\"openai\",\n    )\n    agent = Agent(\n        prompt.model_identifier, # \"openai:gpt-4o\"\n        system_instructions=prompt.system_instruction[0].unwrap(),\n    )\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Prompt.model_settings","title":"<code>model_settings</code>  <code>property</code>","text":"<p>The model settings to use for the prompt.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Prompt.provider","title":"<code>provider</code>  <code>property</code>","text":"<p>The provider to use for the prompt.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Prompt.response_json_schema","title":"<code>response_json_schema</code>  <code>property</code>","text":"<p>The JSON schema for the response if provided.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Prompt.system_instruction","title":"<code>system_instruction</code>  <code>property</code>","text":"<p>The system message to use in the prompt.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Prompt.__init__","title":"<code>__init__(message, model, provider, system_instruction=None, model_settings=None, response_format=None)</code>","text":"<p>Prompt for interacting with an LLM API.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | Sequence[str | ImageUrl | AudioUrl | BinaryContent | DocumentUrl] | Message | List[Message]</code> <p>The prompt to use.</p> required <code>model</code> <code>str</code> <p>The model to use for the prompt</p> required <code>provider</code> <code>Provider | str</code> <p>The provider to use for the prompt.</p> required <code>system_instruction</code> <code>Optional[str | List[str]]</code> <p>The system prompt to use in the prompt.</p> <code>None</code> <code>model_settings</code> <code>None</code> <p>The model settings to use for the prompt. Defaults to None which means no model settings will be used</p> <code>None</code> <code>response_format</code> <code>Optional[BaseModel | Score]</code> <p>The response format to use for the prompt. This is used for Structured Outputs (https://platform.openai.com/docs/guides/structured-outputs?api-mode=chat). Currently, response_format only support Pydantic BaseModel classes and the PotatoHead Score class. The provided response_format will be parsed into a JSON schema.</p> <code>None</code> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def __init__(\n    self,\n    message: (\n        str\n        | Sequence[str | ImageUrl | AudioUrl | BinaryContent | DocumentUrl]\n        | Message\n        | List[Message]\n        | List[Dict[str, Any]]\n    ),\n    model: str,\n    provider: Provider | str,\n    system_instruction: Optional[str | List[str]] = None,\n    model_settings: Optional[ModelSettings | OpenAIChatSettings | GeminiSettings] = None,\n    response_format: Optional[Any] = None,\n) -&gt; None:\n    \"\"\"Prompt for interacting with an LLM API.\n\n    Args:\n        message (str | Sequence[str | ImageUrl | AudioUrl | BinaryContent | DocumentUrl] | Message | List[Message]):\n            The prompt to use.\n        model (str):\n            The model to use for the prompt\n        provider (Provider | str):\n            The provider to use for the prompt.\n        system_instruction (Optional[str | List[str]]):\n            The system prompt to use in the prompt.\n        model_settings (None):\n            The model settings to use for the prompt.\n            Defaults to None which means no model settings will be used\n        response_format (Optional[BaseModel | Score]):\n            The response format to use for the prompt. This is used for Structured Outputs\n            (https://platform.openai.com/docs/guides/structured-outputs?api-mode=chat).\n            Currently, response_format only support Pydantic BaseModel classes and the PotatoHead Score class.\n            The provided response_format will be parsed into a JSON schema.\n\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Prompt.bind","title":"<code>bind(name=None, value=None, **kwargs)</code>","text":"<p>Bind context to a specific variable in the prompt. This is an immutable operation meaning that it will return a new Prompt object with the context bound. This will iterate over all user messages.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the variable to bind.</p> <code>None</code> <code>value</code> <code>str | int | float | bool | list</code> <p>The value to bind the variable to. Must be a JSON serializable type.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to bind to the prompt. This can be used to bind multiple variables at once.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>The prompt with the context bound.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def bind(\n    self,\n    name: Optional[str] = None,\n    value: Optional[str | int | float | bool | list] = None,\n    **kwargs: Any,\n) -&gt; \"Prompt\":\n    \"\"\"Bind context to a specific variable in the prompt. This is an immutable operation meaning that it\n    will return a new Prompt object with the context bound. This will iterate over all user messages.\n\n    Args:\n        name (str):\n            The name of the variable to bind.\n        value (str | int | float | bool | list):\n            The value to bind the variable to. Must be a JSON serializable type.\n        **kwargs (Any):\n            Additional keyword arguments to bind to the prompt. This can be used to bind multiple variables at once.\n\n    Returns:\n        Prompt:\n            The prompt with the context bound.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Prompt.bind_mut","title":"<code>bind_mut(name=None, value=None, **kwargs)</code>","text":"<p>Bind context to a specific variable in the prompt. This is a mutable operation meaning that it will modify the current Prompt object. This will iterate over all user messages.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the variable to bind.</p> <code>None</code> <code>value</code> <code>str | int | float | bool | list</code> <p>The value to bind the variable to. Must be a JSON serializable type.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to bind to the prompt. This can be used to bind multiple variables at once.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>The prompt with the context bound.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def bind_mut(\n    self,\n    name: Optional[str] = None,\n    value: Optional[str | int | float | bool | list] = None,\n    **kwargs: Any,\n) -&gt; \"Prompt\":\n    \"\"\"Bind context to a specific variable in the prompt. This is a mutable operation meaning that it\n    will modify the current Prompt object. This will iterate over all user messages.\n\n    Args:\n        name (str):\n            The name of the variable to bind.\n        value (str | int | float | bool | list):\n            The value to bind the variable to. Must be a JSON serializable type.\n        **kwargs (Any):\n            Additional keyword arguments to bind to the prompt. This can be used to bind multiple variables at once.\n\n    Returns:\n        Prompt:\n            The prompt with the context bound.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Prompt.from_path","title":"<code>from_path(path)</code>  <code>staticmethod</code>","text":"<p>Load a prompt from a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the prompt file.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>The loaded prompt.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>@staticmethod\ndef from_path(path: Path) -&gt; \"Prompt\":\n    \"\"\"Load a prompt from a file.\n\n    Args:\n        path (Path):\n            The path to the prompt file.\n\n    Returns:\n        Prompt:\n            The loaded prompt.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Prompt.model_dump_json","title":"<code>model_dump_json()</code>","text":"<p>Dump the model to a JSON string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The JSON string.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def model_dump_json(self) -&gt; str:\n    \"\"\"Dump the model to a JSON string.\n\n    Returns:\n        str:\n            The JSON string.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Prompt.model_validate_json","title":"<code>model_validate_json(json_string)</code>  <code>staticmethod</code>","text":"<p>Validate the model JSON.</p> <p>Parameters:</p> Name Type Description Default <code>json_string</code> <code>str</code> <p>The JSON string to validate.</p> required <p>Returns:     Prompt:         The prompt object.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>@staticmethod\ndef model_validate_json(json_string: str) -&gt; \"Prompt\":\n    \"\"\"Validate the model JSON.\n\n    Args:\n        json_string (str):\n            The JSON string to validate.\n    Returns:\n        Prompt:\n            The prompt object.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Prompt.save_prompt","title":"<code>save_prompt(path=None)</code>","text":"<p>Save the prompt to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[Path]</code> <p>The path to save the prompt to. If None, the prompt will be saved to the current working directory.</p> <code>None</code> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def save_prompt(self, path: Optional[Path] = None) -&gt; None:\n    \"\"\"Save the prompt to a file.\n\n    Args:\n        path (Optional[Path]):\n            The path to save the prompt to. If None, the prompt will be saved to\n            the current working directory.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.PromptTokenDetails","title":"<code>PromptTokenDetails</code>","text":"<p>Details about the prompt tokens used in a request.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class PromptTokenDetails:\n    \"\"\"Details about the prompt tokens used in a request.\"\"\"\n\n    @property\n    def audio_tokens(self) -&gt; int:\n        \"\"\"The number of audio tokens used in the request.\"\"\"\n\n    @property\n    def cached_tokens(self) -&gt; int:\n        \"\"\"The number of cached tokens used in the request.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.PromptTokenDetails.audio_tokens","title":"<code>audio_tokens</code>  <code>property</code>","text":"<p>The number of audio tokens used in the request.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.PromptTokenDetails.cached_tokens","title":"<code>cached_tokens</code>  <code>property</code>","text":"<p>The number of cached tokens used in the request.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.PyTask","title":"<code>PyTask</code>","text":"<p>Python-specific task interface for Task objects and results</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class PyTask:\n    \"\"\"Python-specific task interface for Task objects and results\"\"\"\n\n    @property\n    def prompt(self) -&gt; Prompt:\n        \"\"\"The prompt to use for the task.\"\"\"\n\n    @property\n    def dependencies(self) -&gt; List[str]:\n        \"\"\"The dependencies of the task.\"\"\"\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"The ID of the task.\"\"\"\n\n    @property\n    def agent_id(self) -&gt; str:\n        \"\"\"The ID of the agent that will execute the task.\"\"\"\n\n    @property\n    def status(self) -&gt; TaskStatus:\n        \"\"\"The status of the task.\"\"\"\n\n    @property\n    def result(self) -&gt; Optional[AgentResponse]:\n        \"\"\"The result of the task if it has been executed, otherwise None.\"\"\"\n\n    def __str__(self) -&gt; str: ...\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.PyTask.agent_id","title":"<code>agent_id</code>  <code>property</code>","text":"<p>The ID of the agent that will execute the task.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.PyTask.dependencies","title":"<code>dependencies</code>  <code>property</code>","text":"<p>The dependencies of the task.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.PyTask.id","title":"<code>id</code>  <code>property</code>","text":"<p>The ID of the task.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.PyTask.prompt","title":"<code>prompt</code>  <code>property</code>","text":"<p>The prompt to use for the task.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.PyTask.result","title":"<code>result</code>  <code>property</code>","text":"<p>The result of the task if it has been executed, otherwise None.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.PyTask.status","title":"<code>status</code>  <code>property</code>","text":"<p>The status of the task.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.ResponseLogProbs","title":"<code>ResponseLogProbs</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class ResponseLogProbs:\n    @property\n    def token(self) -&gt; str:\n        \"\"\"The token for which the log probabilities are calculated.\"\"\"\n\n    @property\n    def logprob(self) -&gt; float:\n        \"\"\"The log probability of the token.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.ResponseLogProbs.logprob","title":"<code>logprob</code>  <code>property</code>","text":"<p>The log probability of the token.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.ResponseLogProbs.token","title":"<code>token</code>  <code>property</code>","text":"<p>The token for which the log probabilities are calculated.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Score","title":"<code>Score</code>","text":"<p>A class representing a score with a score value and a reason. This is typically used as a response type for tasks/prompts that require scoring or evaluation of results.</p> <p>Example: <pre><code>    Prompt(\n        model=\"openai:gpt-4o\",\n        message=\"What is the score of this response?\",\n        system_instruction=\"system_prompt\",\n        response_format=Score,\n    )\n</code></pre></p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class Score:\n    \"\"\"A class representing a score with a score value and a reason. This is typically used\n    as a response type for tasks/prompts that require scoring or evaluation of results.\n\n    Example:\n    ```python\n        Prompt(\n            model=\"openai:gpt-4o\",\n            message=\"What is the score of this response?\",\n            system_instruction=\"system_prompt\",\n            response_format=Score,\n        )\n    ```\n    \"\"\"\n\n    @property\n    def score(self) -&gt; int:\n        \"\"\"The score value.\"\"\"\n\n    @property\n    def reason(self) -&gt; str:\n        \"\"\"The reason for the score.\"\"\"\n\n    @staticmethod\n    def model_validate_json(json_string: str) -&gt; \"Score\":\n        \"\"\"Validate the score JSON.\n\n        Args:\n            json_string (str):\n                The JSON string to validate.\n\n        Returns:\n            Score:\n                The score object.\n        \"\"\"\n\n    def __str__(self): ...\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Score.reason","title":"<code>reason</code>  <code>property</code>","text":"<p>The reason for the score.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Score.score","title":"<code>score</code>  <code>property</code>","text":"<p>The score value.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Score.model_validate_json","title":"<code>model_validate_json(json_string)</code>  <code>staticmethod</code>","text":"<p>Validate the score JSON.</p> <p>Parameters:</p> Name Type Description Default <code>json_string</code> <code>str</code> <p>The JSON string to validate.</p> required <p>Returns:</p> Name Type Description <code>Score</code> <code>Score</code> <p>The score object.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>@staticmethod\ndef model_validate_json(json_string: str) -&gt; \"Score\":\n    \"\"\"Validate the score JSON.\n\n    Args:\n        json_string (str):\n            The JSON string to validate.\n\n    Returns:\n        Score:\n            The score object.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Task","title":"<code>Task</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class Task:\n    def __init__(\n        self,\n        agent_id: str,\n        prompt: Prompt,\n        dependencies: List[str] = [],\n        id: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Create a Task object.\n\n        Args:\n            agent_id (str):\n                The ID of the agent that will execute the task.\n            prompt (Prompt):\n                The prompt to use for the task.\n            dependencies (List[str]):\n                The dependencies of the task.\n            id (Optional[str]):\n                The ID of the task. If None, a random uuid7 will be generated.\n        \"\"\"\n\n    @property\n    def prompt(self) -&gt; Prompt:\n        \"\"\"The prompt to use for the task.\"\"\"\n\n    @property\n    def dependencies(self) -&gt; List[str]:\n        \"\"\"The dependencies of the task.\"\"\"\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"The ID of the task.\"\"\"\n\n    @property\n    def status(self) -&gt; TaskStatus:\n        \"\"\"The status of the task.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Task.dependencies","title":"<code>dependencies</code>  <code>property</code>","text":"<p>The dependencies of the task.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Task.id","title":"<code>id</code>  <code>property</code>","text":"<p>The ID of the task.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Task.prompt","title":"<code>prompt</code>  <code>property</code>","text":"<p>The prompt to use for the task.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Task.status","title":"<code>status</code>  <code>property</code>","text":"<p>The status of the task.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Task.__init__","title":"<code>__init__(agent_id, prompt, dependencies=[], id=None)</code>","text":"<p>Create a Task object.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>str</code> <p>The ID of the agent that will execute the task.</p> required <code>prompt</code> <code>Prompt</code> <p>The prompt to use for the task.</p> required <code>dependencies</code> <code>List[str]</code> <p>The dependencies of the task.</p> <code>[]</code> <code>id</code> <code>Optional[str]</code> <p>The ID of the task. If None, a random uuid7 will be generated.</p> <code>None</code> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def __init__(\n    self,\n    agent_id: str,\n    prompt: Prompt,\n    dependencies: List[str] = [],\n    id: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Create a Task object.\n\n    Args:\n        agent_id (str):\n            The ID of the agent that will execute the task.\n        prompt (Prompt):\n            The prompt to use for the task.\n        dependencies (List[str]):\n            The dependencies of the task.\n        id (Optional[str]):\n            The ID of the task. If None, a random uuid7 will be generated.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.TaskEvent","title":"<code>TaskEvent</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class TaskEvent:\n    @property\n    def id(self) -&gt; str:\n        \"\"\"The ID of the event\"\"\"\n\n    @property\n    def workflow_id(self) -&gt; str:\n        \"\"\"The ID of the workflow that the task is part of.\"\"\"\n\n    @property\n    def task_id(self) -&gt; str:\n        \"\"\"The ID of the task that the event is associated with.\"\"\"\n\n    @property\n    def status(self) -&gt; TaskStatus:\n        \"\"\"The status of the task at the time of the event.\"\"\"\n\n    @property\n    def timestamp(self) -&gt; datetime.datetime:\n        \"\"\"The timestamp of the event. This is the time when the event occurred.\"\"\"\n\n    @property\n    def updated_at(self) -&gt; datetime.datetime:\n        \"\"\"The timestamp of when the event was last updated. This is useful for tracking changes to the event.\"\"\"\n\n    @property\n    def details(self) -&gt; EventDetails:\n        \"\"\"Additional details about the event. This can include information such as error messages or other relevant data.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.TaskEvent.details","title":"<code>details</code>  <code>property</code>","text":"<p>Additional details about the event. This can include information such as error messages or other relevant data.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.TaskEvent.id","title":"<code>id</code>  <code>property</code>","text":"<p>The ID of the event</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.TaskEvent.status","title":"<code>status</code>  <code>property</code>","text":"<p>The status of the task at the time of the event.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.TaskEvent.task_id","title":"<code>task_id</code>  <code>property</code>","text":"<p>The ID of the task that the event is associated with.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.TaskEvent.timestamp","title":"<code>timestamp</code>  <code>property</code>","text":"<p>The timestamp of the event. This is the time when the event occurred.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.TaskEvent.updated_at","title":"<code>updated_at</code>  <code>property</code>","text":"<p>The timestamp of when the event was last updated. This is useful for tracking changes to the event.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.TaskEvent.workflow_id","title":"<code>workflow_id</code>  <code>property</code>","text":"<p>The ID of the workflow that the task is part of.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.TaskList","title":"<code>TaskList</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class TaskList:\n    def __init__(self) -&gt; None:\n        \"\"\"Create a TaskList object.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.TaskList.__init__","title":"<code>__init__()</code>","text":"<p>Create a TaskList object.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Create a TaskList object.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Usage","title":"<code>Usage</code>","text":"<p>Usage statistics for a model response.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class Usage:\n    \"\"\"Usage statistics for a model response.\"\"\"\n\n    @property\n    def completion_tokens(self) -&gt; int:\n        \"\"\"The number of completion tokens used in the response.\"\"\"\n\n    @property\n    def prompt_tokens(self) -&gt; int:\n        \"\"\"The number of prompt tokens used in the request.\"\"\"\n\n    @property\n    def total_tokens(self) -&gt; int:\n        \"\"\"The total number of tokens used in the request and response.\"\"\"\n\n    @property\n    def completion_tokens_details(self) -&gt; CompletionTokenDetails:\n        \"\"\"Details about the completion tokens used in the response.\"\"\"\n\n    @property\n    def prompt_tokens_details(self) -&gt; \"PromptTokenDetails\":\n        \"\"\"Details about the prompt tokens used in the request.\"\"\"\n\n    @property\n    def finish_reason(self) -&gt; str:\n        \"\"\"The reason why the model stopped generating tokens\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Usage.completion_tokens","title":"<code>completion_tokens</code>  <code>property</code>","text":"<p>The number of completion tokens used in the response.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Usage.completion_tokens_details","title":"<code>completion_tokens_details</code>  <code>property</code>","text":"<p>Details about the completion tokens used in the response.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Usage.finish_reason","title":"<code>finish_reason</code>  <code>property</code>","text":"<p>The reason why the model stopped generating tokens</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Usage.prompt_tokens","title":"<code>prompt_tokens</code>  <code>property</code>","text":"<p>The number of prompt tokens used in the request.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Usage.prompt_tokens_details","title":"<code>prompt_tokens_details</code>  <code>property</code>","text":"<p>Details about the prompt tokens used in the request.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Usage.total_tokens","title":"<code>total_tokens</code>  <code>property</code>","text":"<p>The total number of tokens used in the request and response.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Workflow","title":"<code>Workflow</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class Workflow:\n    def __init__(self, name: str) -&gt; None:\n        \"\"\"Create a Workflow object.\n\n        Args:\n            name (str):\n                The name of the workflow.\n        \"\"\"\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"The name of the workflow.\"\"\"\n\n    @property\n    def task_list(self) -&gt; TaskList:\n        \"\"\"The tasks in the workflow.\"\"\"\n\n    @property\n    def agents(self) -&gt; Dict[str, Agent]:\n        \"\"\"The agents in the workflow.\"\"\"\n\n    @property\n    def is_workflow(self) -&gt; bool:\n        \"\"\"Returns True if the workflow is a valid workflow, otherwise False.\n        This is used to determine if the workflow can be executed.\n        \"\"\"\n\n    def __workflow__(self) -&gt; str:\n        \"\"\"Returns a string representation of the workflow.\"\"\"\n\n    def add_task_output_types(self, task_output_types: Dict[str, Any]) -&gt; None:\n        \"\"\"Add output types for tasks in the workflow. This is primarily used for\n        when loading a workflow as python objects are not serializable.\n\n        Args:\n            task_output_types (Dict[str, Any]):\n                A dictionary mapping task IDs to their output types.\n                This can either be a Pydantic `BaseModel` class or a supported potato_head response type such as `Score`.\n        \"\"\"\n\n    def add_task(self, task: Task, output_type: Optional[Any]) -&gt; None:\n        \"\"\"Add a task to the workflow.\n\n        Args:\n            task (Task):\n                The task to add to the workflow.\n            output_type (Optional[Any]):\n                The output type to use for the task. This can either be a Pydantic `BaseModel` class\n                or a supported potato_head response type such as `Score`.\n        \"\"\"\n\n    def add_tasks(self, tasks: List[Task]) -&gt; None:\n        \"\"\"Add multiple tasks to the workflow.\n\n        Args:\n            tasks (List[Task]):\n                The tasks to add to the workflow.\n        \"\"\"\n\n    def add_agent(self, agent: Agent) -&gt; None:\n        \"\"\"Add an agent to the workflow.\n\n        Args:\n            agent (Agent):\n                The agent to add to the workflow.\n        \"\"\"\n\n    def is_complete(self) -&gt; bool:\n        \"\"\"Check if the workflow is complete.\n\n        Returns:\n            bool:\n                True if the workflow is complete, False otherwise.\n        \"\"\"\n\n    def pending_count(self) -&gt; int:\n        \"\"\"Get the number of pending tasks in the workflow.\n\n        Returns:\n            int:\n                The number of pending tasks in the workflow.\n        \"\"\"\n\n    def execution_plan(self) -&gt; Dict[str, List[str]]:\n        \"\"\"Get the execution plan for the workflow.\n\n        Returns:\n            Dict[str, List[str]]:\n                A dictionary where the keys are task IDs and the values are lists of task IDs\n                that the task depends on.\n        \"\"\"\n\n    def run(\n        self,\n        global_context: Optional[Dict[str, Any]] = None,\n    ) -&gt; WorkflowResult:\n        \"\"\"Run the workflow. This will execute all tasks in the workflow and return when all tasks are complete.\n\n        Args:\n            global_context (Optional[Dict[str, Any]]):\n                A dictionary of global context to bind to the workflow.\n                All tasks in the workflow will have this context bound to them.\n        \"\"\"\n\n    def model_dump_json(self) -&gt; str:\n        \"\"\"Dump the workflow to a JSON string.\n\n        Returns:\n            str:\n                The JSON string.\n        \"\"\"\n\n    @staticmethod\n    def model_validate_json(json_string: str, output_types: Optional[Dict[str, Any]]) -&gt; \"Workflow\":\n        \"\"\"Load a workflow from a JSON string.\n\n        Args:\n            json_string (str):\n                The JSON string to validate.\n            output_types (Optional[Dict[str, Any]]):\n                A dictionary mapping task IDs to their output types.\n                This can either be a Pydantic `BaseModel` class or a supported potato_head response type such as `Score`.\n\n        Returns:\n            Workflow:\n                The workflow object.\n        \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Workflow.agents","title":"<code>agents</code>  <code>property</code>","text":"<p>The agents in the workflow.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Workflow.is_workflow","title":"<code>is_workflow</code>  <code>property</code>","text":"<p>Returns True if the workflow is a valid workflow, otherwise False. This is used to determine if the workflow can be executed.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Workflow.name","title":"<code>name</code>  <code>property</code>","text":"<p>The name of the workflow.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Workflow.task_list","title":"<code>task_list</code>  <code>property</code>","text":"<p>The tasks in the workflow.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Workflow.__init__","title":"<code>__init__(name)</code>","text":"<p>Create a Workflow object.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the workflow.</p> required Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def __init__(self, name: str) -&gt; None:\n    \"\"\"Create a Workflow object.\n\n    Args:\n        name (str):\n            The name of the workflow.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Workflow.__workflow__","title":"<code>__workflow__()</code>","text":"<p>Returns a string representation of the workflow.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def __workflow__(self) -&gt; str:\n    \"\"\"Returns a string representation of the workflow.\"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Workflow.add_agent","title":"<code>add_agent(agent)</code>","text":"<p>Add an agent to the workflow.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>The agent to add to the workflow.</p> required Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def add_agent(self, agent: Agent) -&gt; None:\n    \"\"\"Add an agent to the workflow.\n\n    Args:\n        agent (Agent):\n            The agent to add to the workflow.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Workflow.add_task","title":"<code>add_task(task, output_type)</code>","text":"<p>Add a task to the workflow.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>The task to add to the workflow.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type to use for the task. This can either be a Pydantic <code>BaseModel</code> class or a supported potato_head response type such as <code>Score</code>.</p> required Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def add_task(self, task: Task, output_type: Optional[Any]) -&gt; None:\n    \"\"\"Add a task to the workflow.\n\n    Args:\n        task (Task):\n            The task to add to the workflow.\n        output_type (Optional[Any]):\n            The output type to use for the task. This can either be a Pydantic `BaseModel` class\n            or a supported potato_head response type such as `Score`.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Workflow.add_task_output_types","title":"<code>add_task_output_types(task_output_types)</code>","text":"<p>Add output types for tasks in the workflow. This is primarily used for when loading a workflow as python objects are not serializable.</p> <p>Parameters:</p> Name Type Description Default <code>task_output_types</code> <code>Dict[str, Any]</code> <p>A dictionary mapping task IDs to their output types. This can either be a Pydantic <code>BaseModel</code> class or a supported potato_head response type such as <code>Score</code>.</p> required Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def add_task_output_types(self, task_output_types: Dict[str, Any]) -&gt; None:\n    \"\"\"Add output types for tasks in the workflow. This is primarily used for\n    when loading a workflow as python objects are not serializable.\n\n    Args:\n        task_output_types (Dict[str, Any]):\n            A dictionary mapping task IDs to their output types.\n            This can either be a Pydantic `BaseModel` class or a supported potato_head response type such as `Score`.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Workflow.add_tasks","title":"<code>add_tasks(tasks)</code>","text":"<p>Add multiple tasks to the workflow.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>List[Task]</code> <p>The tasks to add to the workflow.</p> required Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def add_tasks(self, tasks: List[Task]) -&gt; None:\n    \"\"\"Add multiple tasks to the workflow.\n\n    Args:\n        tasks (List[Task]):\n            The tasks to add to the workflow.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Workflow.execution_plan","title":"<code>execution_plan()</code>","text":"<p>Get the execution plan for the workflow.</p> <p>Returns:</p> Type Description <code>Dict[str, List[str]]</code> <p>Dict[str, List[str]]: A dictionary where the keys are task IDs and the values are lists of task IDs that the task depends on.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def execution_plan(self) -&gt; Dict[str, List[str]]:\n    \"\"\"Get the execution plan for the workflow.\n\n    Returns:\n        Dict[str, List[str]]:\n            A dictionary where the keys are task IDs and the values are lists of task IDs\n            that the task depends on.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Workflow.is_complete","title":"<code>is_complete()</code>","text":"<p>Check if the workflow is complete.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the workflow is complete, False otherwise.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def is_complete(self) -&gt; bool:\n    \"\"\"Check if the workflow is complete.\n\n    Returns:\n        bool:\n            True if the workflow is complete, False otherwise.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Workflow.model_dump_json","title":"<code>model_dump_json()</code>","text":"<p>Dump the workflow to a JSON string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The JSON string.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def model_dump_json(self) -&gt; str:\n    \"\"\"Dump the workflow to a JSON string.\n\n    Returns:\n        str:\n            The JSON string.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Workflow.model_validate_json","title":"<code>model_validate_json(json_string, output_types)</code>  <code>staticmethod</code>","text":"<p>Load a workflow from a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>json_string</code> <code>str</code> <p>The JSON string to validate.</p> required <code>output_types</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary mapping task IDs to their output types. This can either be a Pydantic <code>BaseModel</code> class or a supported potato_head response type such as <code>Score</code>.</p> required <p>Returns:</p> Name Type Description <code>Workflow</code> <code>Workflow</code> <p>The workflow object.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>@staticmethod\ndef model_validate_json(json_string: str, output_types: Optional[Dict[str, Any]]) -&gt; \"Workflow\":\n    \"\"\"Load a workflow from a JSON string.\n\n    Args:\n        json_string (str):\n            The JSON string to validate.\n        output_types (Optional[Dict[str, Any]]):\n            A dictionary mapping task IDs to their output types.\n            This can either be a Pydantic `BaseModel` class or a supported potato_head response type such as `Score`.\n\n    Returns:\n        Workflow:\n            The workflow object.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Workflow.pending_count","title":"<code>pending_count()</code>","text":"<p>Get the number of pending tasks in the workflow.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of pending tasks in the workflow.</p> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def pending_count(self) -&gt; int:\n    \"\"\"Get the number of pending tasks in the workflow.\n\n    Returns:\n        int:\n            The number of pending tasks in the workflow.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.Workflow.run","title":"<code>run(global_context=None)</code>","text":"<p>Run the workflow. This will execute all tasks in the workflow and return when all tasks are complete.</p> <p>Parameters:</p> Name Type Description Default <code>global_context</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary of global context to bind to the workflow. All tasks in the workflow will have this context bound to them.</p> <code>None</code> Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>def run(\n    self,\n    global_context: Optional[Dict[str, Any]] = None,\n) -&gt; WorkflowResult:\n    \"\"\"Run the workflow. This will execute all tasks in the workflow and return when all tasks are complete.\n\n    Args:\n        global_context (Optional[Dict[str, Any]]):\n            A dictionary of global context to bind to the workflow.\n            All tasks in the workflow will have this context bound to them.\n    \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.WorkflowResult","title":"<code>WorkflowResult</code>","text":"Source code in <code>python/potato_head/_potato_head.pyi</code> <pre><code>class WorkflowResult:\n    @property\n    def tasks(self) -&gt; Dict[str, PyTask]:\n        \"\"\"The tasks in the workflow result.\"\"\"\n\n    @property\n    def events(self) -&gt; List[TaskEvent]:\n        \"\"\"The events that occurred during the workflow execution. This is a list of dictionaries\n        where each dictionary contains information about the event such as the task ID, status, and timestamp.\n        \"\"\"\n</code></pre>"},{"location":"docs/api/potato_head/#potato_head._potato_head.WorkflowResult.events","title":"<code>events</code>  <code>property</code>","text":"<p>The events that occurred during the workflow execution. This is a list of dictionaries where each dictionary contains information about the event such as the task ID, status, and timestamp.</p>"},{"location":"docs/api/potato_head/#potato_head._potato_head.WorkflowResult.tasks","title":"<code>tasks</code>  <code>property</code>","text":"<p>The tasks in the workflow result.</p>"}]}